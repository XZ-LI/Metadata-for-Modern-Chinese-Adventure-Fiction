{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fa3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: A Floating City.txt\n",
      "Processing file: A Sub And A Submarine The Story Of H.M. Submarine R19 In The Great War.txt\n",
      "Processing file: Adrift In The Wilds; Or, The Adventures Of Two Shipwrecked Boys.txt\n",
      "Processing file: Allan Quatermain.txt\n",
      "Processing file: An Aerial Bivouac.txt\n",
      "Processing file: Ayesha, The Return Of She.txt\n",
      "Processing file: A_Sailor_s_Bride.txt\n",
      "Processing file: Benita.txt\n",
      "Processing file: Black Heart and White Heart and Other Stories.txt\n",
      "Processing file: By Pike And Dyke A Tale Of The Rise Of The Dutch Republic.txt\n",
      "Processing file: By The Gods Beloved.txt\n",
      "Processing file: Cleopatra.txt\n",
      "Processing file: Eighty_Days.txt\n",
      "Processing file: Eric Brighteyes.txt\n",
      "Processing file: Fair Margaret.txt\n",
      "Processing file: Gulliver's Travels into Several Remote Nations of the World.txt\n",
      "Processing file: His Tramps and Troubles Told by Himself.txt\n",
      "Processing file: Jack Manly.txt\n",
      "Processing file: Jimmy_Brown_Trying_to_Find_Europe.txt\n",
      "Processing file: King Solomon'S Mines.txt\n",
      "Processing file: Maiwa's Revenge, or The War of the Little Hand.txt\n",
      "Processing file: Masterman Ready; Or, The Wreck Of The Pacific.txt\n",
      "Processing file: Montezuma's Daughter.txt\n",
      "Processing file: Mr Meeson's Will.txt\n",
      "Processing file: MS. found in a Bottle.txt\n",
      "Processing file: Oliver Twist .txt\n",
      "Processing file: Orange and Green.txt\n",
      "Processing file: Peter The Whaler.txt\n",
      "Processing file: Queen Sheba'S Ring.txt\n",
      "Processing file: robinson crusoe.txt\n",
      "Processing file: She.txt\n",
      "Processing file: THE ADVENTURE OF THE MASON.txt\n",
      "Processing file: The Boy Emigrants.txt\n",
      "Processing file: The Further Adventures of Robinson Crusoe.txt\n",
      "Processing file: The Ghost Kings.txt\n",
      "Processing file: The History Of Sindbad, The Sailor.txt\n",
      "Processing file: The Jewel of Seven Stars.txt\n",
      "Processing file: The Lost World.txt\n",
      "Processing file: The People Of The Mist.txt\n",
      "Processing file: The Story Of Sindbad The Voyager.txt\n",
      "Processing file: The Surprising Adventures Of Baron Munchausen.txt\n",
      "Processing file: The Swiss Family Robinson in Words of One Syllable.txt\n",
      "Processing file: The Wanderers; Or, Adventures in the Wilds of Trinidad and Orinoco.txt\n",
      "Processing file: The Witch's Head.txt\n",
      "Processing file: The World's Desire.txt\n",
      "Processing file: The_Story_of_the_Sea.txt\n",
      "Processing file: Ticket no.txt\n",
      "Processing file: Treasure Island.txt\n",
      "Location frequencies with entity types have been written to location_frequencies.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Load the spaCy model and set a higher max_length limit if necessary\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000  # Adjust as needed for large texts\n",
    "\n",
    "# Define a function to split text into smaller chunks\n",
    "def split_text(text, chunk_size=100000):\n",
    "    \"\"\"Splits text into chunks of a specified size.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Define the folder containing the text files\n",
    "folder_path = 'English'\n",
    "\n",
    "# Initialize a defaultdict to track frequencies and types of named entities\n",
    "location_counter = defaultdict(lambda: {'Frequency': 0, 'Type': ''})\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Only process text files\n",
    "        print(f\"Processing file: {filename}\")  # Print progress\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Check if the text length exceeds the max_length and split if necessary\n",
    "            if len(text) > nlp.max_length:\n",
    "                text_chunks = split_text(text)\n",
    "            else:\n",
    "                text_chunks = [text]\n",
    "            \n",
    "            # Process each chunk individually\n",
    "            for chunk in text_chunks:\n",
    "                doc = nlp(chunk)  # Process the chunk with spaCy\n",
    "                \n",
    "                # Extract named entities and update the counter for GPE, LOC, and NORP only\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ in [\"GPE\", \"LOC\", \"NORP\"]:  # Check for GPE, LOC, or NORP labels\n",
    "                        location_counter[ent.text]['Frequency'] += 1\n",
    "                        location_counter[ent.text]['Type'] = ent.label_\n",
    "\n",
    "# Convert the defaultdict to a DataFrame\n",
    "location_data = [{'Location': loc, 'Frequency': data['Frequency'], 'Type': data['Type']} \n",
    "                 for loc, data in location_counter.items()]\n",
    "location_df = pd.DataFrame(location_data)\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "location_df.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "\n",
    "# Output the results to an Excel file\n",
    "output_file = 'location_frequencies.xlsx'\n",
    "location_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Location frequencies with entity types have been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938dbf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Autour de la Lune.txt\n",
      "Processing file: Cinq Semaines En Ballon.txt\n",
      "Processing file: De la Terre à la Lune.txt\n",
      "Processing file: Famille-sans-nom.txt\n",
      "Processing file: L'Lle Mysterieuse.txt\n",
      "Processing file: La maison à vapeur.txt\n",
      "Processing file: La_Vénus_noire.txt\n",
      "Processing file: Le Tour Du Monde En Quatre-Vingts Jours.txt\n",
      "Processing file: LES TROIS MOUSQUETAIRES.txt\n",
      "Processing file: Michel Strogoff de Moscou à Irkoutsk .txt\n",
      "Processing file: Un Billet de loterie.txt\n",
      "Processing file: Un-dirigeable-au-pôle-nord-Émile-Driant-2015-BnF-Partenariats-9782346010363-55bc7c3be6791153da0a827f.txt\n",
      "Processing file: Une Affaire Mystérieuse.txt\n",
      "Processing file: Une Ville Flottante.txt\n",
      "Processing file: Verne-vacances.txt\n",
      "Processing file: Vingt Ans Après.txt\n",
      "Processing file: Vingt Mille Lieues Sous Les Mers.txt\n",
      "Processing file: Voyage Au Centre De La Terre.txt\n",
      "Location frequencies with entity types have been written to location_frequencies_French.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Load the French spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "nlp.max_length = 2000000  # Adjust as needed for large texts\n",
    "\n",
    "# Define a function to split text into smaller chunks\n",
    "def split_text(text, chunk_size=100000):\n",
    "    \"\"\"Splits text into chunks of a specified size.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Define the folder containing the text files\n",
    "folder_path = 'French_cleaned'\n",
    "\n",
    "# Initialize a defaultdict to track frequencies and types of named entities\n",
    "location_counter = defaultdict(lambda: {'Frequency': 0, 'Type': ''})\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Only process text files\n",
    "        print(f\"Processing file: {filename}\")  # Print progress\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "            # Check if the text length exceeds the max_length and split if necessary\n",
    "            if len(text) > nlp.max_length:\n",
    "                text_chunks = split_text(text)\n",
    "            else:\n",
    "                text_chunks = [text]\n",
    "            \n",
    "            # Process each chunk individually\n",
    "            for chunk in text_chunks:\n",
    "                doc = nlp(chunk)  # Process the chunk with spaCy\n",
    "                \n",
    "                # Extract named entities and update the counter for GPE, LOC, and NORP only\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ in [\"GPE\", \"LOC\", \"NORP\"]:  # Check for GPE, LOC, or NORP labels\n",
    "                        location_counter[ent.text]['Frequency'] += 1\n",
    "                        location_counter[ent.text]['Type'] = ent.label_\n",
    "\n",
    "# Convert the defaultdict to a DataFrame\n",
    "location_data = [{'Location': loc, 'Frequency': data['Frequency'], 'Type': data['Type']} \n",
    "                 for loc, data in location_counter.items()]\n",
    "location_df = pd.DataFrame(location_data)\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "location_df.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "\n",
    "# Output the results to an Excel file\n",
    "output_file = 'location_frequencies_French.xlsx'\n",
    "location_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Location frequencies with entity types have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29510a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            phrase  frequency  \\\n",
      "0  中國/中華/支那/中/中華民國        298   \n",
      "1         日本/日本國/日        105   \n",
      "2               古巴         57   \n",
      "3      英國/英/英倫/英吉利         74   \n",
      "4               希臘         39   \n",
      "\n",
      "                                         translation  \n",
      "0  Translated(src=zh-TW, dest=en, text=China/Chin...  \n",
      "1  Translated(src=ja, dest=en, text=Japan/Japan/d...  \n",
      "2  Translated(src=zh-CN, dest=en, text=Cuba, pron...  \n",
      "3  Translated(src=zh-TW, dest=en, text=British/Br...  \n",
      "4  Translated(src=zh-TW, dest=en, text=Greece, pr...  \n",
      "Updated DataFrame with translations has been saved to all_freq_country_translated.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "# Specify the input file path\n",
    "input_file_path = 'all_freq_country.xlsx'\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Initialize the GoogleTranslator for French to English\n",
    "translator = Translator()\n",
    "\n",
    "# Translate the content of the 'phrase' column and store it in the 'translation' column\n",
    "df['translation'] = df['phrase'].apply(lambda x: translator.translate(x,src='auto', dest='en') if pd.notnull(x) else x)\n",
    "\n",
    "# Print the updated DataFrame (or use display in Jupyter Notebook)\n",
    "print(df.head())\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = 'all_freq_country_translated.xlsx'\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated DataFrame with translations has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356d8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cities  frequency  Translation translation\n",
      "0         倫敦        665          NaN      London\n",
      "1  巴黎/巴黎市/勃黎        620          NaN       Paris\n",
      "2     紐約/紐約克        186          NaN         New\n",
      "3         開羅        167          NaN       Cairo\n",
      "4     羅馬/羅馬城        162          NaN        Rome\n",
      "Updated DataFrame with translations has been saved to all_freq_trans_cities_translated.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from time import sleep\n",
    "\n",
    "# Specify the input file path\n",
    "input_file_path = 'all_freq_trans_cities.xlsx'\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Initialize the GoogleTranslator for French to English\n",
    "translator = Translator()\n",
    "\n",
    "# Function to safely translate the first word before a \"/\" and handle errors\n",
    "def safe_translate_first_word(text):\n",
    "    try:\n",
    "        if pd.notnull(text):\n",
    "            # Extract the first word before any \"/\"\n",
    "            first_word = text.split('/')[0].split()[0]\n",
    "            # Translate the first word\n",
    "            translation = translator.translate(first_word, src='auto', dest='en')\n",
    "            return translation.text.split()[0]  # Ensure only one English word is returned\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating '{text}': {e}\")\n",
    "        return text\n",
    "\n",
    "# Apply the function to the 'phrase' column and store it in the 'translation' column\n",
    "df['translation'] = df['Cities'].apply(safe_translate_first_word)\n",
    "\n",
    "# Print the updated DataFrame (or use display in Jupyter Notebook)\n",
    "print(df.head())\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = 'all_freq_trans_cities_translated.xlsx'\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated DataFrame with translations has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2ef68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
