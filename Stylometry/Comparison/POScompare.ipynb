{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd90dfc7-e584-49b1-98a8-2a2ac44f6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-document metrics saved to linshu_mattr_metrics_per_doc.csv\n",
      "By-group averages saved to linshu_mattr_metrics_by_group.csv\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load your JSON\n",
    "with open(\"all_Lin_Shu_token_pos_ner_by_subfolder.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Functions ---\n",
    "def ttr(tokens):\n",
    "    return len(set(tokens)) / len(tokens) if tokens else np.nan\n",
    "\n",
    "def root_ttr(tokens):\n",
    "    return len(set(tokens)) / math.sqrt(len(tokens)) if tokens else np.nan\n",
    "\n",
    "def mattr(tokens, window_size=500):\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    if n < window_size:\n",
    "        return len(set(tokens)) / n\n",
    "    counter = Counter(tokens[:window_size])\n",
    "    uniq_counts = [len(counter)]\n",
    "    for i in range(window_size, n):\n",
    "        out_tok = tokens[i - window_size]\n",
    "        in_tok = tokens[i]\n",
    "        counter[out_tok] -= 1\n",
    "        if counter[out_tok] == 0:\n",
    "            del counter[out_tok]\n",
    "        counter[in_tok] += 1\n",
    "        uniq_counts.append(len(counter))\n",
    "    return np.mean([uc / window_size for uc in uniq_counts])\n",
    "\n",
    "# --- Compute metrics ---\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    tokens = row[\"tokens\"]\n",
    "    results.append({\n",
    "        \"filename\": row[\"filename\"],\n",
    "        \"group\": row[\"group\"],\n",
    "        \"n_tokens\": len(tokens),\n",
    "        \"TTR\": ttr(tokens),\n",
    "        \"Root_TTR\": root_ttr(tokens),\n",
    "        \"MATTR_300\": mattr(tokens, 300),\n",
    "        \"MATTR_500\": mattr(tokens, 500),\n",
    "        \"MATTR_800\": mattr(tokens, 800)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "\n",
    "# --- Aggregate by group ---\n",
    "agg_df = (metrics_df\n",
    "          .groupby(\"group\")\n",
    "          .agg({\"n_tokens\": [\"count\",\"mean\",\"median\"],\n",
    "                \"TTR\": \"mean\",\n",
    "                \"Root_TTR\": \"mean\",\n",
    "                \"MATTR_300\": \"mean\",\n",
    "                \"MATTR_500\": \"mean\",\n",
    "                \"MATTR_800\": \"mean\"})\n",
    "          .reset_index())\n",
    "\n",
    "# Save outputs\n",
    "metrics_df.to_csv(\"linshu_mattr_metrics_per_doc.csv\", index=False)\n",
    "agg_df.to_csv(\"linshu_mattr_metrics_by_group.csv\", index=False)\n",
    "\n",
    "print(\"Per-document metrics saved to linshu_mattr_metrics_per_doc.csv\")\n",
    "print(\"By-group averages saved to linshu_mattr_metrics_by_group.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b75da1-bf1c-4380-891d-f3424a86e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 14257 chunks from 29 texts.\n",
      "count    14257.000000\n",
      "mean       100.003577\n",
      "std          0.237602\n",
      "min         99.000000\n",
      "25%        100.000000\n",
      "50%        100.000000\n",
      "75%        100.000000\n",
      "max        101.000000\n",
      "Name: chunk_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# === Load your JSON file ===\n",
    "with open(\"all_Lin_Shu_token_pos_ner_by_subfolder.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "TARGET_SIZE = 100  # desired avg chunk size\n",
    "\n",
    "def chunk_evenly(tokens, pos_tags, target_size=100):\n",
    "    \"\"\"\n",
    "    Split tokens/pos into K contiguous chunks so that:\n",
    "      - all tokens are used (no drop)\n",
    "      - chunk sizes differ by at most 1\n",
    "      - average size ≈ target_size\n",
    "    \"\"\"\n",
    "    n = len(tokens)\n",
    "    assert n == len(pos_tags), \"tokens and pos length mismatch\"\n",
    "\n",
    "    # Number of chunks ~ n / target_size (at least 1)\n",
    "    k = max(1, round(n / target_size))\n",
    "\n",
    "    # Distribute sizes as evenly as possible: first 'remainder' chunks get +1\n",
    "    base = n // k\n",
    "    rem  = n % k\n",
    "    sizes = [base + 1] * rem + [base] * (k - rem)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for j, sz in enumerate(sizes, 1):\n",
    "        end = start + sz\n",
    "        chunks.append((\n",
    "            j,\n",
    "            tokens[start:end],\n",
    "            pos_tags[start:end]\n",
    "        ))\n",
    "        start = end\n",
    "\n",
    "    # Sanity checks\n",
    "    assert sum(len(c[1]) for c in chunks) == n\n",
    "    return chunks\n",
    "\n",
    "# === Chunking process (no tokens ditched; all chunks ~100) ===\n",
    "chunked_rows = []\n",
    "\n",
    "for entry in data:\n",
    "    tokens   = entry[\"tokens\"]\n",
    "    pos_tags = entry[\"pos\"]\n",
    "    group    = entry[\"group\"]\n",
    "    filename = entry[\"filename\"]\n",
    "\n",
    "    for chunk_id, tok_chunk, pos_chunk in chunk_evenly(tokens, pos_tags, TARGET_SIZE):\n",
    "        chunked_rows.append({\n",
    "            \"filename\": f\"{filename}_chunk{chunk_id}\",\n",
    "            \"group\": group,\n",
    "            \"tokens\": tok_chunk,\n",
    "            \"pos\": pos_chunk,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"chunk_len\": len(tok_chunk),\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(chunked_rows)\n",
    "print(f\"✅ Created {len(df_chunks)} chunks from {len(data)} texts.\")\n",
    "print(df_chunks[\"chunk_len\"].describe())\n",
    "\n",
    "# === Save ===\n",
    "df_chunks.to_json(\"chunked_ave100_token_blocks.json\", force_ascii=False, orient=\"records\", indent=2)\n",
    "df_chunks.to_csv(\"chunked_ave100_token_blocks.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c44e2a8-5131-4e2d-8e22-3e1b72c610a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Relative pronoun frequency by group ===\n",
      "               group  n_chunks  mean_rel  median_rel   std_rel    se_rel  \\\n",
      "0    Original Novels      1211  0.035256        0.03  0.024037  0.000691   \n",
      "1  Translated Novels     13046  0.076142        0.07  0.038198  0.000334   \n",
      "\n",
      "   ci95_low  ci95_high  \n",
      "0  0.033902   0.036610  \n",
      "1  0.075487   0.076798  \n",
      "\n",
      "=== Top 5 chunks per group (by relative pronoun frequency) ===\n",
      "                              filename              group  token_count  \\\n",
      "1917     Aiji jinta poushi ji_chunk707  Translated Novels           99   \n",
      "2424    Aisilan qingxia zhuan_chunk465  Translated Novels          100   \n",
      "8536  Sanqian nian yan shi ji_chunk441  Translated Novels          100   \n",
      "1688     Aiji jinta poushi ji_chunk478  Translated Novels          100   \n",
      "4670         Gu gui yi jin ji_chunk401  Translated Novels          100   \n",
      "1063       Yuan hai ling guang_chunk52    Original Novels          100   \n",
      "556         Jinghua bi xue lu_chunk175    Original Novels           99   \n",
      "699            Jinguo Yangqiu_chunk134    Original Novels          100   \n",
      "1104       Yuan hai ling guang_chunk93    Original Novels          100   \n",
      "1124      Yuan hai ling guang_chunk113    Original Novels          100   \n",
      "\n",
      "      pron_count  pron_rel  \n",
      "1917          24  0.242424  \n",
      "2424          24  0.240000  \n",
      "8536          24  0.240000  \n",
      "1688          24  0.240000  \n",
      "4670          23  0.230000  \n",
      "1063          13  0.130000  \n",
      "556           12  0.121212  \n",
      "699           12  0.120000  \n",
      "1104          12  0.120000  \n",
      "1124          12  0.120000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load chunked data (has: filename, group, tokens, pos, chunk_id, chunk_len)\n",
    "df = pd.read_json(\"chunked_ave100_token_blocks.json\")\n",
    "\n",
    "# 1) Ensure token_count (for normalization)\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"tokens\"].apply(len)\n",
    "\n",
    "# 2) Count pronouns by POS tag (= PN) per chunk\n",
    "PRON_TAG = \"PN\"\n",
    "df[\"pron_count\"] = df[\"pos\"].apply(lambda tags: sum(1 for t in tags if t == PRON_TAG))\n",
    "\n",
    "# 3) Relative pronoun frequency per chunk\n",
    "df[\"pron_rel\"] = df[\"pron_count\"] / df[\"token_count\"]\n",
    "\n",
    "# 4) Compare by group (mean/median/std + 95% CI)\n",
    "group_summary = (\n",
    "    df.groupby(\"group\")\n",
    "      .agg(n_chunks=(\"filename\",\"count\"),\n",
    "           mean_rel=(\"pron_rel\",\"mean\"),\n",
    "           median_rel=(\"pron_rel\",\"median\"),\n",
    "           std_rel=(\"pron_rel\",\"std\"))\n",
    "      .reset_index()\n",
    ")\n",
    "n_per_group = df.groupby(\"group\")[\"pron_rel\"].count().values\n",
    "group_summary[\"se_rel\"] = group_summary[\"std_rel\"] / (n_per_group ** 0.5)\n",
    "group_summary[\"ci95_low\"]  = group_summary[\"mean_rel\"] - 1.96 * group_summary[\"se_rel\"]\n",
    "group_summary[\"ci95_high\"] = group_summary[\"mean_rel\"] + 1.96 * group_summary[\"se_rel\"]\n",
    "\n",
    "print(\"=== Relative pronoun frequency by group ===\")\n",
    "print(group_summary)\n",
    "\n",
    "# 5) Top 5 chunks per group by pronoun density\n",
    "top5 = (\n",
    "    df.sort_values(\"pron_rel\", ascending=False)\n",
    "      .groupby(\"group\", group_keys=False)\n",
    "      .head(5)\n",
    ")[[\"filename\",\"group\",\"token_count\",\"pron_count\",\"pron_rel\"]]\n",
    "\n",
    "print(\"\\n=== Top 5 chunks per group (by relative pronoun frequency) ===\")\n",
    "print(top5)\n",
    "\n",
    "# 6) (Optional) Save outputs\n",
    "# df.to_csv(\"pron_rel_per_chunk.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "# group_summary.to_csv(\"pron_rel_by_group.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65b82023-4381-4510-8134-19329f9ecdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Group: Original Novels ===\n",
      "Filename: Jinghua bi xue lu_chunk126\n",
      "Chunk ID: 126, tokens: 100, function words: 30 (0.300)\n",
      "Function-word POS breakdown: {'SP': 4, 'PN': 11, 'P': 4, 'DT': 2, 'DEG': 2, 'DEC': 2, 'CS': 1, 'LB': 1, 'MSP': 2, 'CC': 1}\n",
      "Function-word tokens (token/POS):\n",
      "也/SP, 我/PN, 以/P, 之/PN, 諸/DT, 於/P, 何/PN, 吾/PN, 之/PN, 以/P, 之/DEG, 我/PN, 於/P, 之/DEC, 我/PN, 之/DEC, 我/PN, 若/CS, 爲/LB, 所/MSP, 矣/SP, 此/DT, 之/DEG, 而/MSP, 我/PN, 此/PN, 我/PN, 也/SP, 然/SP, 及/CC\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk478\n",
      "Chunk ID: 478, tokens: 100, function words: 42 (0.420)\n",
      "Function-word POS breakdown: {'PN': 24, 'SP': 4, 'MSP': 1, 'DT': 2, 'P': 6, 'DEG': 4, 'CS': 1}\n",
      "Function-word tokens (token/POS):\n",
      "何/PN, 也/SP, 余/PN, 爾/PN, 而/MSP, 此/DT, 與/P, 汝/PN, 吾/PN, 之/PN, 吾/PN, 之/PN, 此/DT, 與/P, 汝/PN, 者/SP, 汝/PN, 於/P, 吾/PN, 與/P, 爾/PN, 爾/PN, 吾/PN, 之/DEG, 汝/PN, 之/PN, 汝/PN, 我/PN, 也/SP, 汝/PN, 與/P, 我/PN, 此/PN, 之/DEG, 汝/PN, 之/PN, 耶/SP, 雖然/CS, 汝/PN, 以/P, 之/DEG, 之/DEG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load chunks (needs: filename, group, tokens, pos, chunk_id, chunk_len)\n",
    "df = pd.read_json(\"chunked_ave100_token_blocks.json\")\n",
    "\n",
    "# Your function-word POS tags (exact list you provided)\n",
    "fw_tags = [\n",
    "    \"AS\",\"BA\",\"CC\",\"CS\",\"DEC\",\"DEG\",\"DER\",\"DEV\",\"DT\",\"ETC\",\n",
    "    \"FW\",\"IC\",\"IJ\",\"LB\",\"LC\",\"MSP\",\"PN\",\"P\",\"PU\",\"SB\",\"SP\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Ensure token_count (normalizer)\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"chunk_len\"] if \"chunk_len\" in df.columns else df[\"tokens\"].apply(len)\n",
    "\n",
    "# Count function-word tokens by POS and compute relative frequency\n",
    "df[\"function_word_count\"] = df[\"pos\"].apply(lambda tags: sum(1 for t in tags if t in fw_tags))\n",
    "df[\"function_word_rel\"] = df[\"function_word_count\"] / df[\"token_count\"]\n",
    "\n",
    "# Get the index of the max-rel chunk per group\n",
    "idxmax = df.groupby(\"group\")[\"function_word_rel\"].idxmax()\n",
    "top_chunks = df.loc[idxmax].copy()\n",
    "\n",
    "# Print details for each group's top chunk\n",
    "for _, row in top_chunks.iterrows():\n",
    "    # Collect function-word (token, POS) pairs\n",
    "    fw_pairs = [(tok, tag) for tok, tag in zip(row[\"tokens\"], row[\"pos\"]) if tag in fw_tags]\n",
    "    pos_counts = Counter(tag for _, tag in fw_pairs)\n",
    "\n",
    "    print(f\"=== Group: {row['group']} ===\")\n",
    "    print(f\"Filename: {row['filename']}\")\n",
    "    print(f\"Chunk ID: {row.get('chunk_id')}, tokens: {row['token_count']}, \"\n",
    "          f\"function words: {row['function_word_count']} \"\n",
    "          f\"({row['function_word_rel']:.3f})\")\n",
    "    print(\"Function-word POS breakdown:\", dict(pos_counts))\n",
    "    print(\"Function-word tokens (token/POS):\")\n",
    "    print(\", \".join(f\"{tok}/{tag}\" for tok, tag in fw_pairs))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a69b137f-4eb5-45f2-a34d-8b5819f360e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function-word relative frequency by group ===\n",
      "               group  n_chunks  mean_rel  median_rel   std_rel    se_rel  \\\n",
      "0    Original Novels      1211  0.142143        0.14  0.041543  0.001194   \n",
      "1  Translated Novels     13046  0.204969        0.20  0.051226  0.000448   \n",
      "\n",
      "   ci95_low  ci95_high  \n",
      "0  0.139803   0.144483  \n",
      "1  0.204090   0.205848  \n",
      "\n",
      "=== Top 5 chunks per group (by function-word relative frequency) ===\n",
      "                            filename              group  token_count  \\\n",
      "1688   Aiji jinta poushi ji_chunk478  Translated Novels          100   \n",
      "14090         Zhongru dulou_chunk424  Translated Novels          100   \n",
      "1920   Aiji jinta poushi ji_chunk710  Translated Novels           99   \n",
      "1917   Aiji jinta poushi ji_chunk707  Translated Novels           99   \n",
      "1918   Aiji jinta poushi ji_chunk708  Translated Novels           99   \n",
      "507       Jinghua bi xue lu_chunk126    Original Novels          100   \n",
      "731          Jinguo Yangqiu_chunk166    Original Novels           99   \n",
      "659           Jinguo Yangqiu_chunk94    Original Novels          100   \n",
      "624           Jinguo Yangqiu_chunk59    Original Novels          100   \n",
      "1104     Yuan hai ling guang_chunk93    Original Novels          100   \n",
      "\n",
      "       function_word_count  function_word_rel  \n",
      "1688                    42           0.420000  \n",
      "14090                   41           0.410000  \n",
      "1920                    40           0.404040  \n",
      "1917                    40           0.404040  \n",
      "1918                    39           0.393939  \n",
      "507                     30           0.300000  \n",
      "731                     27           0.272727  \n",
      "659                     27           0.270000  \n",
      "624                     27           0.270000  \n",
      "1104                    27           0.270000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Load chunks ---\n",
    "df = pd.read_json(\"chunked_ave100_token_blocks.json\")\n",
    "\n",
    "# --- Your function-word POS tags (exact set you provided) ---\n",
    "fw_tags = [\n",
    "    \"AS\",\"BA\",\"CC\",\"CS\",\"DEC\",\"DEG\",\"DER\",\"DEV\",\"DT\",\"ETC\",\n",
    "    \"FW\",\"IC\",\"IJ\",\"LB\",\"LC\",\"MSP\",\"P\",\"PN\",\"PU\",\"SB\",\"SP\",\n",
    "    \n",
    "]\n",
    "\n",
    "# --- Token count per chunk (100-token chunks -> equals chunk_len) ---\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"chunk_len\"] if \"chunk_len\" in df.columns else df[\"tokens\"].apply(len)\n",
    "\n",
    "# --- POS counts per chunk -> wide columns POS_AD, POS_VV, ... ---\n",
    "def pos_counter(tags):\n",
    "    c = Counter(map(str, tags))\n",
    "    return {f\"POS_{k}\": v for k, v in c.items()}\n",
    "\n",
    "pos_count_df = df[\"pos\"].apply(pos_counter).apply(pd.Series).fillna(0).astype(int)\n",
    "\n",
    "# --- Attach counts back ---\n",
    "dfc = pd.concat([df.reset_index(drop=True), pos_count_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# --- Sum function-word counts and compute relative frequency ---\n",
    "fw_pos_cols = [f\"POS_{t}\" for t in fw_tags if f\"POS_{t}\" in dfc.columns]\n",
    "dfc[\"function_word_count\"] = dfc[fw_pos_cols].sum(axis=1) if fw_pos_cols else 0\n",
    "dfc[\"function_word_rel\"] = dfc[\"function_word_count\"] / dfc[\"token_count\"]  # with 100-token chunks, this ≈ count/100\n",
    "\n",
    "# (Optional) per-tag relatives for inspection\n",
    "for col in fw_pos_cols:\n",
    "    dfc[col.replace(\"POS_\", \"REL_\")] = dfc[col] / dfc[\"token_count\"]\n",
    "\n",
    "# --- Compare by group ---\n",
    "group_summary = (\n",
    "    dfc.groupby(\"group\")\n",
    "       .agg(n_chunks=(\"filename\",\"count\"),\n",
    "            mean_rel=(\"function_word_rel\",\"mean\"),\n",
    "            median_rel=(\"function_word_rel\",\"median\"),\n",
    "            std_rel=(\"function_word_rel\",\"std\"))\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "# 95% CI for the mean (normal approx)\n",
    "n_per_group = dfc.groupby(\"group\")[\"function_word_rel\"].count()\n",
    "group_summary[\"se_rel\"] = group_summary[\"std_rel\"] / n_per_group.values**0.5\n",
    "group_summary[\"ci95_low\"]  = group_summary[\"mean_rel\"] - 1.96 * group_summary[\"se_rel\"]\n",
    "group_summary[\"ci95_high\"] = group_summary[\"mean_rel\"] + 1.96 * group_summary[\"se_rel\"]\n",
    "\n",
    "print(\"=== Function-word relative frequency by group ===\")\n",
    "print(group_summary)\n",
    "\n",
    "# --- Top 5 chunks per group ---\n",
    "top5 = (\n",
    "    dfc.sort_values(\"function_word_rel\", ascending=False)\n",
    "       .groupby(\"group\", group_keys=False)\n",
    "       .head(5)\n",
    ")\n",
    "print(\"\\n=== Top 5 chunks per group (by function-word relative frequency) ===\")\n",
    "print(top5[[\"filename\",\"group\",\"token_count\",\"function_word_count\",\"function_word_rel\"]])\n",
    "\n",
    "# --- (Optional) Save ---\n",
    "# dfc.to_csv(\"function_word_relfreq_per_chunk_100.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "# group_summary.to_csv(\"function_word_relfreq_by_group_100.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- (Optional) Significance test if exactly two groups ---\n",
    "# from scipy.stats import mannwhitneyu\n",
    "# g = {k: v[\"function_word_rel\"].values for k, v in dfc.groupby(\"group\")}\n",
    "# if len(g) == 2:\n",
    "#     (g1, g2) = list(g.keys())\n",
    "#     print(mannwhitneyu(g[g1], g[g2], alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8899912-80be-4761-87e5-e6a4d70bfea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Group: Original Novels ===\n",
      "Filename: Jinghua bi xue lu_chunk126  |  Chunk ID: 126\n",
      "Tokens in chunk: 100  |  Function words: 30 (0.300)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "妄也我決不戕教且以力衛之聯軍諸將曰於何取證沈曰吾即證之以主教之身主教今日所以得生能訟我於聯軍審判之堂即我保衛之力我若不衛主教久已爲亂民所殺矣何能留此完全之軀命而訟我此即我所以衛主教也諸將鹹以爲然得不坐乃定廷穆死刑廷穆滿洲人恂恂孝友能書畫藏楊椒山手跡及 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "妄 也 我 決 不 戕 教 且 以 力 衛 之 聯軍 諸 將 曰 於 何 取證 沈 曰 吾 即 證 之 以 主教 之 身 主教 今日 所以 得 生 能 訟 我 於 聯軍 審判 之 堂 即 我 保衛 之 力 我 若 不 衛 主教 久已 爲 亂民 所 殺 矣 何 能 留 此 完全 之 軀 命 而 訟 我 此 即 我 所以 衛 主教 也 諸將 鹹 以爲 然 得 不 坐 乃 定 廷 穆 死刑 廷 穆 滿洲 人 恂恂 孝友 能 書畫 藏 楊椒山 手跡 及 \n",
      "\n",
      "— token/POS pairs:\n",
      "妄/VV 也/SP 我/PN 決/AD 不/AD 戕/VV 教/NN 且/AD 以/P 力/NN 衛/VV 之/PN 聯軍/NN 諸/DT 將/NN 曰/VV 於/P 何/PN 取證/VV 沈/NR 曰/VV 吾/PN 即/AD 證/VV 之/PN 以/P 主教/NN 之/DEG 身/NN 主教/NN 今日/NT 所以/AD 得/VV 生/NN 能/VV 訟/VV 我/PN 於/P 聯軍/NN 審判/VV 之/DEC 堂/NN 即/VC 我/PN 保衛/VV 之/DEC 力/NN 我/PN 若/CS 不/AD 衛/VV 主教/NN 久已/AD 爲/LB 亂民/NN 所/MSP 殺/VV 矣/SP 何/AD 能/VV 留/VV 此/DT 完全/JJ 之/DEG 軀/NN 命/NN 而/MSP 訟/VV 我/PN 此/PN 即/AD 我/PN 所以/AD 衛/VV 主教/NN 也/SP 諸將/NN 鹹/AD 以爲/VV 然/SP 得/VV 不/AD 坐/VV 乃/AD 定/VV 廷/NN 穆/NR 死刑/NN 廷/NN 穆/VV 滿洲/NR 人/NN 恂恂/VV 孝友/NN 能/VV 書畫/VV 藏/VV 楊椒山/NR 手跡/NN 及/CC\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk478  |  Chunk ID: 478\n",
      "Tokens in chunk: 100  |  Function words: 42 (0.420)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "何也余曰查美鶯爾今夕胡傾筐倒陂而出此事正無一與汝吾播之吾穫之此事又何與汝者汝果自裁於理奚當矧吾罪與爾適均焉能誅爾吾之禍患卽汝擎之正不能謂汝罪重逾我也汝播惡種收穫與我正同此不孫之物理汝知之耶雖然汝亦太愚以娟媖之心遂舉彌天之禍被 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "何 也 余 曰 查美鶯 爾 今夕 胡 傾 筐 倒 陂 而 出 此 事 正 無 一 與 汝 吾 播 之 吾 穫 之 此 事 又 何 與 汝 者 汝 果 自裁 於 理 奚 當 矧 吾 罪 與 爾 適 均 焉 能 誅 爾 吾 之 禍患 卽 汝 擎 之 正 不能 謂 汝 罪 重 逾 我 也 汝 播 惡種 收穫 與 我 正 同 此 不 孫 之 物理 汝 知 之 耶 雖然 汝 亦 太 愚 以 娟媖 之 心 遂 舉 彌天 之 禍 被 \n",
      "\n",
      "— token/POS pairs:\n",
      "何/PN 也/SP 余/PN 曰/VV 查美鶯/NR 爾/PN 今夕/NT 胡/AD 傾/VV 筐/NN 倒/VV 陂/NN 而/MSP 出/VV 此/DT 事/NN 正/AD 無/VE 一/CD 與/P 汝/PN 吾/PN 播/VV 之/PN 吾/PN 穫/VV 之/PN 此/DT 事/NN 又/AD 何/AD 與/P 汝/PN 者/SP 汝/PN 果/AD 自裁/VV 於/P 理/NN 奚/AD 當/VV 矧/VV 吾/PN 罪/NN 與/P 爾/PN 適/VV 均/AD 焉/AD 能/VV 誅/VV 爾/PN 吾/PN 之/DEG 禍患/NN 卽/AD 汝/PN 擎/VV 之/PN 正/NN 不能/VV 謂/VV 汝/PN 罪/NN 重/VA 逾/VV 我/PN 也/SP 汝/PN 播/VV 惡種/NN 收穫/NN 與/P 我/PN 正/AD 同/VA 此/PN 不/AD 孫/NN 之/DEG 物理/NN 汝/PN 知/VV 之/PN 耶/SP 雖然/CS 汝/PN 亦/AD 太/AD 愚/VA 以/P 娟媖/JJ 之/DEG 心/NN 遂/AD 舉/VV 彌天/JJ 之/DEG 禍/NN 被/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Load chunks (needs: filename, group, tokens, pos, chunk_id, chunk_len) ---\n",
    "df = pd.read_json(\"chunked_100_token_blocks.json\")\n",
    "\n",
    "# --- Function-word POS tags (your exact set) ---\n",
    "fw_tags = [\n",
    "    \"AS\",\"BA\",\"CC\",\"CS\",\"DEC\",\"DEG\",\"DER\",\"DEV\",\"DT\",\"ETC\",\n",
    "    \"FW\",\"IC\",\"IJ\",\"LB\",\"LC\",\"MSP\",\"P\",\"PN\",\"PU\",\"SB\",\"SP\",\n",
    "    \n",
    "]\n",
    "\n",
    "# --- Ensure token_count for normalization ---\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"chunk_len\"] if \"chunk_len\" in df.columns else df[\"tokens\"].apply(len)\n",
    "\n",
    "# --- Compute function-word relative frequency per chunk ---\n",
    "df[\"function_word_count\"] = df[\"pos\"].apply(lambda tags: sum(1 for t in tags if t in fw_tags))\n",
    "df[\"function_word_rel\"] = df[\"function_word_count\"] / df[\"token_count\"]\n",
    "\n",
    "# --- Helper to pretty-print a chunk ---\n",
    "def print_chunk(row):\n",
    "    print(f\"=== Group: {row['group']} ===\")\n",
    "    print(f\"Filename: {row['filename']}  |  Chunk ID: {row.get('chunk_id')}\")\n",
    "    print(f\"Tokens in chunk: {row['token_count']}  |  Function words: {row['function_word_count']} \"\n",
    "          f\"({row['function_word_rel']:.3f})\\n\")\n",
    "\n",
    "    # Print the whole chunk as text (Chinese: no spaces) and with spaces (for inspection)\n",
    "    text_no_space = \"\".join(row[\"tokens\"])\n",
    "    text_with_space = \" \".join(row[\"tokens\"])\n",
    "\n",
    "    print(\"— Chunk as continuous text (no spaces):\")\n",
    "    print(text_no_space, \"\\n\")\n",
    "    print(\"— Chunk with spaces (for inspection):\")\n",
    "    print(text_with_space, \"\\n\")\n",
    "\n",
    "    # Print all token/POS pairs\n",
    "    print(\"— token/POS pairs:\")\n",
    "    print(\" \".join(f\"{tok}/{tag}\" for tok, tag in zip(row[\"tokens\"], row[\"pos\"])))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# --- Find and print the top chunk per group ---\n",
    "idxmax = df.groupby(\"group\")[\"function_word_rel\"].idxmax()\n",
    "top_chunks = df.loc[idxmax]\n",
    "\n",
    "for _, row in top_chunks.iterrows():\n",
    "    print_chunk(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebaf10e-e9d8-4845-99d7-0ca061f4976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-novel (token-weighted) pronoun rate — first rows:\n",
      "                        novel_id              group  n_chunks  sum_pron  \\\n",
      "0           Aiji jinta poushi ji  Translated Novels       749      6536   \n",
      "1          Aisilan qingxia zhuan  Translated Novels       577      4498   \n",
      "2             Chan chao ji shang  Translated Novels       506      4178   \n",
      "3                Chan chao ji xu  Translated Novels       469      3554   \n",
      "4  Feizhou yanshui chou cheng lu  Translated Novels       758      5351   \n",
      "\n",
      "   sum_tok  mean_of_chunks  pron_rate_weighted  \n",
      "0    74854        0.087325            0.087317  \n",
      "1    57743        0.077900            0.077897  \n",
      "2    50637        0.082506            0.082509  \n",
      "3    46934        0.075731            0.075723  \n",
      "4    75769        0.070627            0.070623  \n",
      "\n",
      "Group-level summary (novel-level, token-weighted):\n",
      "               group  n_texts  mean_rel  median_rel   std_rel    se_rel  \\\n",
      "0    Original Novels        6  0.035480    0.031462  0.008421  0.003438   \n",
      "1  Translated Novels       23  0.075843    0.075951  0.006443  0.001343   \n",
      "\n",
      "   ci95_low  ci95_high  \n",
      "0  0.026643   0.044317  \n",
      "1  0.073057   0.078629  \n",
      "\n",
      "Welch t-test on pron_rate_weighted: t=-10.935, p=1.79e-05\n",
      "Hedges' g (Original Novels - Translated Novels) on pron_rate_weighted: -5.725\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Load chunked data\n",
    "# -----------------------------\n",
    "# Expects columns: filename, group, tokens, pos, chunk_id, chunk_len\n",
    "df = pd.read_json(\"chunked_ave100_token_blocks.json\")\n",
    "\n",
    "# Ensure token_count\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"chunk_len\"] if \"chunk_len\" in df.columns else df[\"tokens\"].apply(len)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Chunk-level pronoun counts (POS tag = PN)\n",
    "# -----------------------------\n",
    "PRON_TAG = \"PN\"\n",
    "df[\"pron_count\"] = df[\"pos\"].apply(lambda tags: sum(1 for t in tags if t == PRON_TAG))\n",
    "df[\"pron_rel\"]   = df[\"pron_count\"] / df[\"token_count\"]  # (for diagnostics)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Aggregate to PER-NOVEL (token-weighted pronoun rate)\n",
    "#    pron_rate_weighted = total pronouns / total tokens in the novel\n",
    "# -----------------------------\n",
    "def base_id(name: str) -> str:\n",
    "    \"\"\"Strip trailing _chunkNN to recover the novel id.\"\"\"\n",
    "    name = str(name)\n",
    "    m = re.search(r\"(.*)_chunk\\d+$\", name)\n",
    "    return m.group(1) if m else name\n",
    "\n",
    "df[\"novel_id\"] = df[\"filename\"].apply(base_id)\n",
    "\n",
    "per_novel = (\n",
    "    df.groupby([\"novel_id\", \"group\"], as_index=False)\n",
    "      .agg(\n",
    "          n_chunks=(\"filename\", \"count\"),\n",
    "          sum_pron=(\"pron_count\", \"sum\"),\n",
    "          sum_tok=(\"token_count\", \"sum\"),\n",
    "          mean_of_chunks=(\"pron_rel\", \"mean\")  # unweighted mean of chunk proportions (optional)\n",
    "      )\n",
    ")\n",
    "per_novel[\"pron_rate_weighted\"] = per_novel[\"sum_pron\"] / per_novel[\"sum_tok\"]\n",
    "\n",
    "print(\"Per-novel (token-weighted) pronoun rate — first rows:\")\n",
    "print(per_novel.head())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Summarize by GROUP across novels (inference at novel level)\n",
    "# -----------------------------\n",
    "measure = \"pron_rate_weighted\"  # or \"mean_of_chunks\"\n",
    "\n",
    "grp = per_novel.groupby(\"group\")[measure]\n",
    "novel_summary = grp.agg(\n",
    "    n_texts=\"count\",\n",
    "    mean_rel=\"mean\",\n",
    "    median_rel=\"median\",\n",
    "    std_rel=\"std\"\n",
    ").reset_index()\n",
    "\n",
    "# Add SE and 95% CI (t-based if SciPy available; else normal approx)\n",
    "def add_ci_t_or_normal(summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    n = summary[\"n_texts\"].to_numpy()\n",
    "    se = summary[\"std_rel\"].to_numpy() / np.sqrt(n)\n",
    "    try:\n",
    "        from scipy.stats import t\n",
    "        tcrit = t.ppf(0.975, df=np.maximum(n - 1, 1))\n",
    "        ci_low  = summary[\"mean_rel\"].to_numpy() - tcrit * se\n",
    "        ci_high = summary[\"mean_rel\"].to_numpy() + tcrit * se\n",
    "    except Exception:\n",
    "        # Normal approx fallback\n",
    "        z = 1.96\n",
    "        ci_low  = summary[\"mean_rel\"].to_numpy() - z * se\n",
    "        ci_high = summary[\"mean_rel\"].to_numpy() + z * se\n",
    "    summary[\"se_rel\"] = se\n",
    "    summary[\"ci95_low\"] = ci_low\n",
    "    summary[\"ci95_high\"] = ci_high\n",
    "    return summary\n",
    "\n",
    "novel_summary = add_ci_t_or_normal(novel_summary)\n",
    "\n",
    "print(\"\\nGroup-level summary (novel-level, token-weighted):\")\n",
    "print(novel_summary[[\"group\",\"n_texts\",\"mean_rel\",\"median_rel\",\"std_rel\",\"se_rel\",\"ci95_low\",\"ci95_high\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Optional: test & effect size between two groups (novel level)\n",
    "# -----------------------------\n",
    "groups = {g: s[measure].values for g, s in per_novel.groupby(\"group\")}\n",
    "if len(groups) == 2:\n",
    "    (g1, g2) = list(groups.keys())\n",
    "    x, y = groups[g1], groups[g2]\n",
    "\n",
    "    # Welch's t-test (does not assume equal variances)\n",
    "    try:\n",
    "        from scipy.stats import ttest_ind\n",
    "        welch = ttest_ind(x, y, equal_var=False)\n",
    "        print(f\"\\nWelch t-test on {measure}: t={welch.statistic:.3f}, p={welch.pvalue:.3g}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Welch t-test skipped: SciPy not available]\", e)\n",
    "\n",
    "    # Hedges' g (bias-corrected Cohen's d)\n",
    "    sx, sy = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    nx, ny = len(x), len(y)\n",
    "    sp = np.sqrt(((nx-1)*sx**2 + (ny-1)*sy**2) / max(nx+ny-2, 1))\n",
    "    d = (np.mean(x) - np.mean(y)) / sp if sp > 0 else np.nan\n",
    "    J = 1 - (3 / (4*(nx+ny) - 9)) if (nx+ny) > 9 else 1.0\n",
    "    g_hedges = J * d\n",
    "    print(f\"Hedges' g ({g1} - {g2}) on {measure}: {g_hedges:.3f}\")\n",
    "else:\n",
    "    print(\"\\nTwo-sample test skipped (need exactly two groups).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Optional: save outputs\n",
    "# -----------------------------\n",
    "# per_novel.to_csv(\"per_novel_pronoun_weighted.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "# novel_summary.to_csv(\"group_summary_novel_level_pronouns_weighted.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8802b2e-489d-4064-9ac8-73af5df4c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Group: Original Novels ===\n",
      "Filename: Yuan hai ling guang_chunk52  |  Chunk ID: 52\n",
      "Tokens in chunk: 100  |  Function words: 13 (0.130)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "也於是夫婦合詞再三請婦躍起曰爾何知今日人挾其僞義以我爲俎上肉恣其分啖所恃者外家有人爲平其曲直今爾夫婦亦爲之揚波而助瀾試問今日人利吾產即畀以產設更趣吾命爾亦坐聽吾死耶紛呶既久日且晡議仍未定匠氏告槥成且殮而嗣事猶懸伯亢聲曰吾固貧 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "也 於是 夫婦 合 詞 再三 請 婦 躍起 曰 爾 何 知 今日 人 挾 其 僞 義 以 我 爲 俎 上 肉 恣 其 分 啖 所 恃 者 外家 有人 爲 平 其 曲直 今 爾 夫婦 亦 爲 之 揚 波 而 助 瀾 試問 今日 人 利 吾 產 即 畀 以 產 設 更 趣 吾 命 爾 亦 坐 聽 吾 死 耶 紛 呶 既 久 日 且 晡 議 仍未 定 匠 氏 告 槥 成 且 殮 而 嗣 事 猶 懸 伯 亢 聲 曰 吾 固 貧 \n",
      "\n",
      "— token/POS pairs:\n",
      "也/SP 於是/AD 夫婦/NN 合/VV 詞/NN 再三/AD 請/VV 婦/NN 躍起/VV 曰/VV 爾/PN 何/AD 知/VV 今日/NT 人/NN 挾/VV 其/PN 僞/JJ 義/NN 以/P 我/PN 爲/VV 俎/NN 上/LC 肉/NN 恣/VV 其/PN 分/VV 啖/VV 所/MSP 恃/VV 者/SP 外家/NN 有人/PN 爲/P 平/VV 其/PN 曲直/NN 今/NT 爾/PN 夫婦/NN 亦/AD 爲/P 之/PN 揚/VV 波/NN 而/MSP 助/VV 瀾/NN 試問/VV 今日/NT 人/NN 利/VV 吾/PN 產/NN 即/AD 畀/VV 以/P 產/NN 設/VV 更/AD 趣/NN 吾/PN 命/VV 爾/PN 亦/AD 坐/VV 聽/VV 吾/PN 死/NN 耶/SP 紛/AD 呶/VV 既/AD 久/VA 日/NN 且/AD 晡/VV 議/NN 仍未/AD 定/VV 匠/NR 氏/NN 告/VV 槥/VV 成/VV 且/AD 殮/VV 而/AD 嗣/NN 事/NN 猶/AD 懸/VV 伯/NN 亢/NR 聲/NN 曰/VV 吾/PN 固/AD 貧/VA\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk707  |  Chunk ID: 707\n",
      "Tokens in chunk: 99  |  Function words: 24 (0.242)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "乎汝其侮我矣余曰格魯巴亞聽之汝臨命近矣此非吾侮殆大神譴汝汝相吾面誰耶汝以為吾黔黑之面跛蹩之何來者非愁怨所集何由至是汝亦知吾果為誰格魯巴亞注目而視狀如野人顫聲言曰吾知汝矣以神譴言之汝得毋為夏馬之夏馬之汝死久奈何得生意其爲厲弄我啣 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "乎 汝 其 侮 我 矣 余 曰 格魯巴亞 聽 之 汝 臨 命 近 矣 此 非 吾 侮 殆 大神 譴 汝 汝 相 吾 面 誰 耶 汝 以為 吾 黔 黑 之 面 跛 蹩 之 何 來 者 非 愁 怨 所 集 何 由 至 是 汝 亦 知 吾 果 為 誰 格魯巴亞 注目 而 視 狀 如 野人 顫 聲 言 曰 吾 知 汝 矣 以 神 譴 言 之 汝 得 毋 為 夏馬 之 夏馬 之 汝 死 久 奈何 得 生意 其 爲 厲 弄 我 啣 \n",
      "\n",
      "— token/POS pairs:\n",
      "乎/SP 汝/PN 其/PN 侮/VV 我/PN 矣/SP 余/PN 曰/VV 格魯巴亞/NR 聽/VV 之/PN 汝/PN 臨/VV 命/NN 近/VA 矣/SP 此/PN 非/VC 吾/PN 侮/VV 殆/VV 大神/NN 譴/VV 汝/PN 汝/PN 相/VV 吾/PN 面/VV 誰/PN 耶/SP 汝/PN 以為/VV 吾/PN 黔/NR 黑/NR 之/DEG 面/NN 跛/VV 蹩/VV 之/SP 何/DT 來/VV 者/SP 非/AD 愁/NN 怨/NN 所/MSP 集/VV 何/AD 由/NN 至/VV 是/VC 汝/PN 亦/AD 知/VV 吾/PN 果/AD 為/VC 誰/PN 格魯巴亞/NR 注目/VV 而/MSP 視/VV 狀/NN 如/P 野人/NN 顫/VV 聲/NN 言/VV 曰/VV 吾/PN 知/VV 汝/PN 矣/SP 以/P 神/NN 譴/VV 言/VV 之/PN 汝/PN 得/VV 毋/AD 為/VC 夏馬/NR 之/DEG 夏馬/NR 之/DEG 汝/PN 死/VV 久/AD 奈何/AD 得/VV 生意/NN 其/PN 爲/P 厲/VV 弄/VV 我/PN 啣/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Load chunks (needs: filename, group, tokens, pos, chunk_id, chunk_len) ---\n",
    "df = pd.read_json(\"chunked_ave100_token_blocks.json\")\n",
    "\n",
    "# --- Function-word POS tags (your exact set) ---\n",
    "fw_tags = [\n",
    "    \"PN\",\n",
    "    \n",
    "]\n",
    "\n",
    "# --- Ensure token_count for normalization ---\n",
    "if \"token_count\" not in df.columns:\n",
    "    df[\"token_count\"] = df[\"chunk_len\"] if \"chunk_len\" in df.columns else df[\"tokens\"].apply(len)\n",
    "\n",
    "# --- Compute function-word relative frequency per chunk ---\n",
    "df[\"function_word_count\"] = df[\"pos\"].apply(lambda tags: sum(1 for t in tags if t in fw_tags))\n",
    "df[\"function_word_rel\"] = df[\"function_word_count\"] / df[\"token_count\"]\n",
    "\n",
    "# --- Helper to pretty-print a chunk ---\n",
    "def print_chunk(row):\n",
    "    print(f\"=== Group: {row['group']} ===\")\n",
    "    print(f\"Filename: {row['filename']}  |  Chunk ID: {row.get('chunk_id')}\")\n",
    "    print(f\"Tokens in chunk: {row['token_count']}  |  Function words: {row['function_word_count']} \"\n",
    "          f\"({row['function_word_rel']:.3f})\\n\")\n",
    "\n",
    "    # Print the whole chunk as text (Chinese: no spaces) and with spaces (for inspection)\n",
    "    text_no_space = \"\".join(row[\"tokens\"])\n",
    "    text_with_space = \" \".join(row[\"tokens\"])\n",
    "\n",
    "    print(\"— Chunk as continuous text (no spaces):\")\n",
    "    print(text_no_space, \"\\n\")\n",
    "    print(\"— Chunk with spaces (for inspection):\")\n",
    "    print(text_with_space, \"\\n\")\n",
    "\n",
    "    # Print all token/POS pairs\n",
    "    print(\"— token/POS pairs:\")\n",
    "    print(\" \".join(f\"{tok}/{tag}\" for tok, tag in zip(row[\"tokens\"], row[\"pos\"])))\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# --- Find and print the top chunk per group ---\n",
    "idxmax = df.groupby(\"group\")[\"function_word_rel\"].idxmax()\n",
    "top_chunks = df.loc[idxmax]\n",
    "\n",
    "for _, row in top_chunks.iterrows():\n",
    "    print_chunk(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebb3b81-322b-4efa-a203-0ecdecb5a939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Top 5 chunks in group: Translated Novels\n",
      "================================================================================\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk707  |  Chunk ID: 707\n",
      "Tokens in chunk: 99  |  Function words: 24 (0.242)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "乎汝其侮我矣余曰格魯巴亞聽之汝臨命近矣此非吾侮殆大神譴汝汝相吾面誰耶汝以為吾黔黑之面跛蹩之何來者非愁怨所集何由至是汝亦知吾果為誰格魯巴亞注目而視狀如野人顫聲言曰吾知汝矣以神譴言之汝得毋為夏馬之夏馬之汝死久奈何得生意其爲厲弄我啣 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "乎 汝 其 侮 我 矣 余 曰 格魯巴亞 聽 之 汝 臨 命 近 矣 此 非 吾 侮 殆 大神 譴 汝 汝 相 吾 面 誰 耶 汝 以為 吾 黔 黑 之 面 跛 蹩 之 何 來 者 非 愁 怨 所 集 何 由 至 是 汝 亦 知 吾 果 為 誰 格魯巴亞 注目 而 視 狀 如 野人 顫 聲 言 曰 吾 知 汝 矣 以 神 譴 言 之 汝 得 毋 為 夏馬 之 夏馬 之 汝 死 久 奈何 得 生意 其 爲 厲 弄 我 啣 \n",
      "\n",
      "— token/POS pairs:\n",
      "乎/SP 汝/PN 其/PN 侮/VV 我/PN 矣/SP 余/PN 曰/VV 格魯巴亞/NR 聽/VV 之/PN 汝/PN 臨/VV 命/NN 近/VA 矣/SP 此/PN 非/VC 吾/PN 侮/VV 殆/VV 大神/NN 譴/VV 汝/PN 汝/PN 相/VV 吾/PN 面/VV 誰/PN 耶/SP 汝/PN 以為/VV 吾/PN 黔/NR 黑/NR 之/DEG 面/NN 跛/VV 蹩/VV 之/SP 何/DT 來/VV 者/SP 非/AD 愁/NN 怨/NN 所/MSP 集/VV 何/AD 由/NN 至/VV 是/VC 汝/PN 亦/AD 知/VV 吾/PN 果/AD 為/VC 誰/PN 格魯巴亞/NR 注目/VV 而/MSP 視/VV 狀/NN 如/P 野人/NN 顫/VV 聲/NN 言/VV 曰/VV 吾/PN 知/VV 汝/PN 矣/SP 以/P 神/NN 譴/VV 言/VV 之/PN 汝/PN 得/VV 毋/AD 為/VC 夏馬/NR 之/DEG 夏馬/NR 之/DEG 汝/PN 死/VV 久/AD 奈何/AD 得/VV 生意/NN 其/PN 爲/P 厲/VV 弄/VV 我/PN 啣/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk478  |  Chunk ID: 478\n",
      "Tokens in chunk: 100  |  Function words: 24 (0.240)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "何也余曰查美鶯爾今夕胡傾筐倒陂而出此事正無一與汝吾播之吾穫之此事又何與汝者汝果自裁於理奚當矧吾罪與爾適均焉能誅爾吾之禍患卽汝擎之正不能謂汝罪重逾我也汝播惡種收穫與我正同此不孫之物理汝知之耶雖然汝亦太愚以娟媖之心遂舉彌天之禍被 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "何 也 余 曰 查美鶯 爾 今夕 胡 傾 筐 倒 陂 而 出 此 事 正 無 一 與 汝 吾 播 之 吾 穫 之 此 事 又 何 與 汝 者 汝 果 自裁 於 理 奚 當 矧 吾 罪 與 爾 適 均 焉 能 誅 爾 吾 之 禍患 卽 汝 擎 之 正 不能 謂 汝 罪 重 逾 我 也 汝 播 惡種 收穫 與 我 正 同 此 不 孫 之 物理 汝 知 之 耶 雖然 汝 亦 太 愚 以 娟媖 之 心 遂 舉 彌天 之 禍 被 \n",
      "\n",
      "— token/POS pairs:\n",
      "何/PN 也/SP 余/PN 曰/VV 查美鶯/NR 爾/PN 今夕/NT 胡/AD 傾/VV 筐/NN 倒/VV 陂/NN 而/MSP 出/VV 此/DT 事/NN 正/AD 無/VE 一/CD 與/P 汝/PN 吾/PN 播/VV 之/PN 吾/PN 穫/VV 之/PN 此/DT 事/NN 又/AD 何/AD 與/P 汝/PN 者/SP 汝/PN 果/AD 自裁/VV 於/P 理/NN 奚/AD 當/VV 矧/VV 吾/PN 罪/NN 與/P 爾/PN 適/VV 均/AD 焉/AD 能/VV 誅/VV 爾/PN 吾/PN 之/DEG 禍患/NN 卽/AD 汝/PN 擎/VV 之/PN 正/NN 不能/VV 謂/VV 汝/PN 罪/NN 重/VA 逾/VV 我/PN 也/SP 汝/PN 播/VV 惡種/NN 收穫/NN 與/P 我/PN 正/AD 同/VA 此/PN 不/AD 孫/NN 之/DEG 物理/NN 汝/PN 知/VV 之/PN 耶/SP 雖然/CS 汝/PN 亦/AD 太/AD 愚/VA 以/P 娟媖/JJ 之/DEG 心/NN 遂/AD 舉/VV 彌天/JJ 之/DEG 禍/NN 被/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aisilan qingxia zhuan_chunk465  |  Chunk ID: 465\n",
      "Tokens in chunk: 100  |  Function words: 24 (0.240)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "之故破吾婚約於願殊未足乎爾胡來愛力克曰爾方呼吾名今奈何復速吾去歌特羅達日爾何為能得吾私語且吾並未呼汝汝殺吾兄吾乃呼汝自近耶汝趣行不則吾將以家眾來殺汝矣愛力克日君儘集其家人至吾非畏死者吾今日自墨司山來卽戴吾頭來耳爾卽不呼吾將代爾集之歌特羅達日爾言何 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "之 故 破 吾 婚約 於 願 殊 未 足 乎 爾 胡 來 愛力克 曰 爾 方 呼 吾 名 今 奈何 復 速 吾 去 歌特羅達日爾 何 為 能 得 吾 私語 且 吾 並 未 呼 汝 汝 殺 吾 兄 吾 乃 呼 汝 自 近 耶 汝 趣 行 不 則 吾 將 以 家眾 來 殺 汝 矣 愛力克 日 君 儘 集 其 家人 至 吾 非 畏 死者 吾 今日 自 墨司山 來 卽 戴 吾 頭 來 耳 爾 卽 不 呼 吾 將 代 爾 集 之 歌特羅達日爾 言 何 \n",
      "\n",
      "— token/POS pairs:\n",
      "之/DEG 故/NN 破/VV 吾/PN 婚約/NN 於/P 願/NN 殊/AD 未/AD 足/VA 乎/SP 爾/SP 胡/NR 來/VV 愛力克/NR 曰/VV 爾/PN 方/AD 呼/VV 吾/PN 名/NN 今/NT 奈何/AD 復/AD 速/VV 吾/PN 去/VV 歌特羅達日爾/NR 何/AD 為/VC 能/VV 得/VV 吾/PN 私語/NN 且/CC 吾/PN 並/AD 未/AD 呼/VV 汝/PN 汝/PN 殺/VV 吾/PN 兄/NN 吾/PN 乃/AD 呼/VV 汝/PN 自/AD 近/VV 耶/SP 汝/PN 趣/VV 行/VV 不/AD 則/AD 吾/PN 將/AD 以/P 家眾/NN 來/MSP 殺/VV 汝/PN 矣/SP 愛力克/NR 日/NR 君/PN 儘/AD 集/VV 其/PN 家人/NN 至/VV 吾/PN 非/AD 畏/VV 死者/NN 吾/PN 今日/NT 自/P 墨司山/NR 來/VV 卽/AD 戴/VV 吾/PN 頭/NN 來/VV 耳/SP 爾/PN 卽/AD 不/AD 呼/VV 吾/PN 將/BA 代/VV 爾/PN 集/VV 之/PN 歌特羅達日爾/NR 言/VV 何/PN\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Sanqian nian yan shi ji_chunk441  |  Chunk ID: 441\n",
      "Tokens in chunk: 100  |  Function words: 24 (0.240)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "無人如女王者王忽曰何利汝遠觀吾態或不了了當就吾旁觀之汝當省記汝身屬汝幸勿責我汝之生愛慕心鹹汝自取汝膽力操守都隳亦汝之過我不惑汝汝今來前吾非趣爾贊我但曰吾果美與否且勿趣言言之無味汝當逐處詳觀觀我形體及我狀貌余手余足余髮余膚一言蔽之汝亦 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "無 人 如 女王 者 王 忽 曰 何 利 汝 遠觀 吾 態 或 不 了了 當 就 吾 旁觀 之 汝 當 省 記 汝 身 屬 汝 幸 勿 責 我 汝 之 生 愛慕 心 鹹 汝 自取 汝 膽力 操守 都 隳 亦 汝 之 過 我 不惑 汝 汝 今 來 前 吾 非 趣 爾 贊 我 但 曰 吾 果 美 與否 且 勿 趣 言 言 之 無味 汝 當 逐 處 詳 觀 觀 我 形體 及 我 狀貌 余 手 余 足 余 髮 余 膚 一言蔽之 汝 亦 \n",
      "\n",
      "— token/POS pairs:\n",
      "無/VE 人/NN 如/AD 女王/NN 者/NN 王/NR 忽/AD 曰/VV 何/DT 利/NN 汝/PN 遠觀/VV 吾/PN 態/NN 或/AD 不/AD 了了/VV 當/NT 就/VV 吾/PN 旁觀/VV 之/DEC 汝/PN 當/AD 省/VV 記/NN 汝/PN 身/NN 屬/VV 汝/PN 幸/VV 勿/AD 責/VV 我/PN 汝/PN 之/DEG 生/NN 愛慕/NN 心/NN 鹹/VA 汝/PN 自取/VV 汝/PN 膽力/NN 操守/NN 都/AD 隳/NN 亦/AD 汝/PN 之/DEG 過/NN 我/PN 不惑/VV 汝/PN 汝/PN 今/NT 來/VV 前/LC 吾/PN 非/AD 趣/VV 爾/PN 贊/VV 我/PN 但/AD 曰/VV 吾/PN 果/AD 美/VA 與否/AD 且/AD 勿/AD 趣/VV 言/VV 言/NN 之/PN 無味/NN 汝/PN 當/AD 逐/P 處/NN 詳/AD 觀/VV 觀/VV 我/PN 形體/NN 及/CC 我/PN 狀貌/NN 余/PN 手/NN 余/NN 足/NN 余/DT 髮/NN 余/NN 膚/NN 一言蔽之/VV 汝/PN 亦/AD\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Translated Novels ===\n",
      "Filename: Aiji jinta poushi ji_chunk139  |  Chunk ID: 139\n",
      "Tokens in chunk: 100  |  Function words: 23 (0.230)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "爾以百媚之情蠱我而我之心志絕迅不留仍不奪吾壯往之志爾難乎其為吾友也以目中見象視之爾亦難信雖然爾識之爾欲如何者請肆其毒螫吾不汝畏設爾欲舉指以撓吾大計則爾之性命即在呼吸今與爾言者已盡於此爾意云何余言時性甚哮勃而查美鶯聞之愈縮 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "爾 以 百媚 之 情 蠱 我 而 我 之 心志 絕 迅 不 留 仍 不 奪 吾 壯 往 之 志 爾 難 乎 其 為 吾 友 也 以 目中 見 象 視 之 爾 亦 難 信 雖然 爾 識 之 爾 欲 如何 者 請 肆 其 毒 螫 吾 不 汝 畏 設 爾 欲 舉 指 以 撓 吾 大計 則 爾 之 性命 即 在 呼吸 今 與 爾 言 者 已 盡 於 此 爾 意 云 何 余 言 時 性 甚 哮 勃 而 查美鶯 聞 之 愈 縮 \n",
      "\n",
      "— token/POS pairs:\n",
      "爾/PN 以/P 百媚/VV 之/DEC 情/NN 蠱/VV 我/PN 而/AD 我/PN 之/DEG 心志/NN 絕/AD 迅/VA 不/AD 留/VV 仍/AD 不/AD 奪/VV 吾/PN 壯/JJ 往/NT 之/DEG 志/NN 爾/PN 難/VA 乎/SP 其/PN 為/VC 吾/PN 友/NN 也/SP 以/P 目中/NN 見/VV 象/NN 視/VV 之/PN 爾/PN 亦/AD 難/AD 信/VV 雖然/CS 爾/PN 識/VV 之/PN 爾/PN 欲/VV 如何/VV 者/SP 請/VV 肆/VV 其/PN 毒/NN 螫/VV 吾/PN 不/AD 汝/PN 畏/VV 設/AD 爾/PN 欲/VV 舉/VV 指/NN 以/MSP 撓/VV 吾/PN 大計/NN 則/AD 爾/PN 之/DEG 性命/NN 即/AD 在/AD 呼吸/VV 今/NT 與/P 爾/PN 言/VV 者/NN 已/AD 盡/VV 於/P 此/PN 爾/PN 意/NN 云/VV 何/DT 余/PN 言/VV 時/LC 性/NN 甚/AD 哮/VV 勃/VA 而/MSP 查美鶯/NR 聞/VV 之/PN 愈/AD 縮/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Top 5 chunks in group: Original Novels\n",
      "================================================================================\n",
      "\n",
      "=== Group: Original Novels ===\n",
      "Filename: Yuan hai ling guang_chunk52  |  Chunk ID: 52\n",
      "Tokens in chunk: 100  |  Function words: 13 (0.130)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "也於是夫婦合詞再三請婦躍起曰爾何知今日人挾其僞義以我爲俎上肉恣其分啖所恃者外家有人爲平其曲直今爾夫婦亦爲之揚波而助瀾試問今日人利吾產即畀以產設更趣吾命爾亦坐聽吾死耶紛呶既久日且晡議仍未定匠氏告槥成且殮而嗣事猶懸伯亢聲曰吾固貧 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "也 於是 夫婦 合 詞 再三 請 婦 躍起 曰 爾 何 知 今日 人 挾 其 僞 義 以 我 爲 俎 上 肉 恣 其 分 啖 所 恃 者 外家 有人 爲 平 其 曲直 今 爾 夫婦 亦 爲 之 揚 波 而 助 瀾 試問 今日 人 利 吾 產 即 畀 以 產 設 更 趣 吾 命 爾 亦 坐 聽 吾 死 耶 紛 呶 既 久 日 且 晡 議 仍未 定 匠 氏 告 槥 成 且 殮 而 嗣 事 猶 懸 伯 亢 聲 曰 吾 固 貧 \n",
      "\n",
      "— token/POS pairs:\n",
      "也/SP 於是/AD 夫婦/NN 合/VV 詞/NN 再三/AD 請/VV 婦/NN 躍起/VV 曰/VV 爾/PN 何/AD 知/VV 今日/NT 人/NN 挾/VV 其/PN 僞/JJ 義/NN 以/P 我/PN 爲/VV 俎/NN 上/LC 肉/NN 恣/VV 其/PN 分/VV 啖/VV 所/MSP 恃/VV 者/SP 外家/NN 有人/PN 爲/P 平/VV 其/PN 曲直/NN 今/NT 爾/PN 夫婦/NN 亦/AD 爲/P 之/PN 揚/VV 波/NN 而/MSP 助/VV 瀾/NN 試問/VV 今日/NT 人/NN 利/VV 吾/PN 產/NN 即/AD 畀/VV 以/P 產/NN 設/VV 更/AD 趣/NN 吾/PN 命/VV 爾/PN 亦/AD 坐/VV 聽/VV 吾/PN 死/NN 耶/SP 紛/AD 呶/VV 既/AD 久/VA 日/NN 且/AD 晡/VV 議/NN 仍未/AD 定/VV 匠/NR 氏/NN 告/VV 槥/VV 成/VV 且/AD 殮/VV 而/AD 嗣/NN 事/NN 猶/AD 懸/VV 伯/NN 亢/NR 聲/NN 曰/VV 吾/PN 固/AD 貧/VA\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Original Novels ===\n",
      "Filename: Jinghua bi xue lu_chunk175  |  Chunk ID: 175\n",
      "Tokens in chunk: 99  |  Function words: 12 (0.121)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "仲符妻欲分其產俾孀妾更立門戶旗人女嫁後殊有力於其外家於是劉乙憑其威力大肆咆勃迨仲光至乙尚出面抗辯仲光曰乙止汝奴於人家乃敢與其家事且汝何力而狂妄如是乙愈怒且以仲光文弱可凌仲光笑曰鼠子敢爾駢二指按其頂乙立屈膝自頓於地仲光曰後此汝當審慎姑氏爲爾惑吾不怨姑氏 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "仲符 妻 欲 分 其 產 俾 孀妾 更 立 門戶 旗人 女 嫁 後 殊 有力 於 其 外家 於是 劉乙 憑 其 威力 大肆 咆勃 迨 仲光 至 乙尚 出面 抗辯 仲光 曰 乙 止 汝 奴 於 人家 乃 敢 與 其 家事 且 汝 何 力 而 狂妄 如是 乙 愈 怒 且 以 仲光 文弱 可 凌 仲光 笑 曰 鼠 子 敢 爾 駢 二 指 按 其 頂 乙 立 屈膝 自 頓 於 地 仲光 曰 後 此 汝 當 審慎 姑 氏 爲 爾 惑 吾 不 怨 姑 氏 \n",
      "\n",
      "— token/POS pairs:\n",
      "仲符/NR 妻/NN 欲/VV 分/VV 其/PN 產/NN 俾/VV 孀妾/NN 更/AD 立/VV 門戶/NN 旗人/NN 女/NN 嫁/VV 後/LC 殊/AD 有力/VA 於/P 其/PN 外家/NN 於是/AD 劉乙/NR 憑/P 其/PN 威力/NN 大肆/AD 咆勃/VV 迨/P 仲光/NR 至/VV 乙尚/NR 出面/VV 抗辯/VV 仲光/NR 曰/VV 乙/NR 止/VV 汝/PN 奴/NN 於/P 人家/PN 乃/AD 敢/VV 與/P 其/PN 家事/VV 且/AD 汝/PN 何/DT 力/NN 而/MSP 狂妄/VA 如是/AD 乙/NR 愈/AD 怒/VV 且/AD 以/P 仲光/NR 文弱/VA 可/VV 凌/VV 仲光/NR 笑/VV 曰/VV 鼠/NN 子/NN 敢/VV 爾/PN 駢/VV 二/CD 指/NN 按/VV 其/PN 頂/NN 乙/NR 立/VV 屈膝/VV 自/AD 頓/VV 於/P 地/NN 仲光/NR 曰/VV 後/AD 此/AD 汝/PN 當/VV 審慎/VA 姑/NR 氏/NN 爲/P 爾/PN 惑/VV 吾/PN 不/AD 怨/VV 姑/NR 氏/NN\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Original Novels ===\n",
      "Filename: Jinguo Yangqiu_chunk134  |  Chunk ID: 134\n",
      "Tokens in chunk: 100  |  Function words: 12 (0.120)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "有物墳起卽含淚不言揮素令出謂良曰汝亦知老身之意乎吾始病時見爾情態似喪爾母者往來奔走如狂吾雖在昏惘有時亦了了見爾躡步於吾榻前顏色莊而且慄色莊者敬若妹也慄者防吾死也爾二人不惟有義而且有禮吾於碧霞元君座前已允以素素嫁汝矣良聞言立跽而 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "有 物 墳 起 卽 含淚 不 言 揮 素 令 出 謂 良 曰 汝 亦 知 老 身 之 意 乎 吾 始 病 時 見 爾 情態 似 喪 爾 母 者 往來 奔走 如 狂 吾 雖 在 昏 惘 有時 亦 了了 見 爾 躡 步 於 吾 榻 前 顏色 莊 而且 慄色 莊 者 敬 若 妹 也 慄 者 防 吾 死 也 爾 二 人 不惟 有 義 而且 有 禮 吾 於 碧霞 元 君 座 前 已 允 以 素素 嫁 汝 矣 良 聞 言 立 跽 而 \n",
      "\n",
      "— token/POS pairs:\n",
      "有/VE 物/NN 墳/VV 起/VV 卽/AD 含淚/VV 不/AD 言/VV 揮/VV 素/NN 令/VV 出/VV 謂/VV 良/NN 曰/VV 汝/PN 亦/AD 知/VV 老/VV 身/NN 之/DEC 意/NN 乎/SP 吾/PN 始/AD 病/VV 時/LC 見/VV 爾/PN 情態/NN 似/AD 喪/VV 爾/PN 母/NN 者/NN 往來/VV 奔走/VV 如/VV 狂/NN 吾/PN 雖/CS 在/AD 昏/VA 惘/VA 有時/AD 亦/AD 了了/VV 見/VV 爾/PN 躡/VV 步/NN 於/P 吾/PN 榻/NN 前/LC 顏色/NN 莊/VA 而且/AD 慄色/NN 莊/NN 者/NN 敬/VV 若/PN 妹/NN 也/SP 慄/NN 者/NN 防/VV 吾/PN 死/VV 也/SP 爾/PN 二/CD 人/NN 不惟/AD 有/VE 義/NN 而且/AD 有/VE 禮/NN 吾/PN 於/P 碧霞/NR 元/NR 君/NN 座/NN 前/LC 已/AD 允/VV 以/P 素素/NR 嫁/VV 汝/PN 矣/SP 良/NR 聞/VV 言/NN 立/VV 跽/NN 而/MSP\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Original Novels ===\n",
      "Filename: Yuan hai ling guang_chunk58  |  Chunk ID: 58\n",
      "Tokens in chunk: 100  |  Function words: 12 (0.120)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "亦縮更一人曰睾丸安在衆反覆數視鹹曰此已足死之矣伯則馳慄不能成聲然一呼鮮血噴溢自口鼻出呼曰哀哉吾弟乃以是死耶婦直前堪伯之胸且批其頰曰賊汝戕爾弟謀吾產尚爲此假惺惺爲吾喪夫尚奚求生趣以刀來並吾同盡則餘產恣爾父子享之足矣伯氣咽不能聲 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "亦 縮 更 一 人 曰 睾丸 安在 衆 反覆 數 視 鹹 曰 此 已 足 死 之 矣 伯 則 馳 慄 不能 成 聲 然 一 呼 鮮血 噴溢 自 口 鼻 出 呼 曰 哀 哉 吾 弟 乃 以 是 死 耶 婦 直 前 堪 伯 之 胸 且 批 其 頰 曰 賊 汝 戕 爾 弟 謀 吾 產 尚 爲此 假惺惺 爲 吾 喪 夫 尚 奚 求 生趣 以 刀 來 並 吾 同 盡 則 餘 產 恣 爾 父子 享 之 足 矣 伯 氣 咽 不能 聲 \n",
      "\n",
      "— token/POS pairs:\n",
      "亦/AD 縮/VV 更/AD 一/CD 人/NN 曰/VV 睾丸/NN 安在/VV 衆/NN 反覆/AD 數/VV 視/VV 鹹/AD 曰/VV 此/PN 已/AD 足/VV 死/VV 之/PN 矣/SP 伯/NN 則/AD 馳/VV 慄/NN 不能/VV 成/VV 聲/NN 然/AD 一/CD 呼/VV 鮮血/NN 噴溢/VV 自/P 口/NN 鼻/NN 出/VV 呼/VV 曰/VV 哀/VV 哉/SP 吾/PN 弟/NN 乃/AD 以/P 是/VC 死/VV 耶/SP 婦/NN 直/AD 前/VV 堪/VV 伯/NR 之/DEG 胸/NN 且/AD 批/VV 其/PN 頰/NN 曰/VV 賊/NN 汝/PN 戕/VV 爾/PN 弟/NN 謀/VV 吾/PN 產/NN 尚/AD 爲此/AD 假惺惺/AD 爲/P 吾/PN 喪/VV 夫/NN 尚/AD 奚/AD 求/VV 生趣/NN 以/P 刀/NN 來/MSP 並/VV 吾/PN 同/AD 盡/VV 則/AD 餘/PN 產/NN 恣/VV 爾/PN 父子/NN 享/VV 之/PN 足/NN 矣/SP 伯/NR 氣/VV 咽/VV 不能/VV 聲/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Group: Original Novels ===\n",
      "Filename: Yuan hai ling guang_chunk93  |  Chunk ID: 93\n",
      "Tokens in chunk: 100  |  Function words: 12 (0.120)\n",
      "\n",
      "— Chunk as continuous text (no spaces):\n",
      "公曰爾確憶得爲此一夕否曰我往時恆以二更睡是夜我幹阿姨同寢姨不可三更後阿姨出我牽裾隨之行姨行甚迅奔向下寨我哭而歸爲外公掌責乃獨寢遲明視阿姨已睡吾側乃不知其何時歸也公曰爾外公知阿姨歸否曰吾未之知公曰阿姨歸時有人同行否曰無之公曰若姨女流烏敢 \n",
      "\n",
      "— Chunk with spaces (for inspection):\n",
      "公 曰 爾 確 憶 得 爲此 一夕 否 曰 我 往 時 恆 以 二更 睡 是 夜 我 幹 阿姨 同 寢 姨 不 可 三更 後 阿姨 出 我 牽 裾 隨 之 行 姨 行 甚 迅 奔向 下 寨 我 哭 而 歸 爲 外公 掌責 乃 獨 寢 遲 明 視 阿姨 已 睡 吾 側 乃 不知 其 何時 歸 也 公 曰 爾 外公 知 阿姨 歸 否 曰 吾 未 之 知 公 曰 阿 姨 歸 時 有人 同行 否 曰 無 之 公 曰 若 姨女 流 烏 敢 \n",
      "\n",
      "— token/POS pairs:\n",
      "公/NN 曰/VV 爾/PN 確/AD 憶/VV 得/DER 爲此/AD 一夕/NN 否/AD 曰/VV 我/PN 往/VV 時/LC 恆/AD 以/P 二更/NT 睡/VV 是/VC 夜/NN 我/PN 幹/P 阿姨/NN 同/AD 寢/NN 姨/NN 不/AD 可/VV 三更/NT 後/LC 阿姨/NN 出/VV 我/PN 牽/VV 裾/NN 隨/P 之/PN 行/VV 姨/NN 行/VV 甚/AD 迅/AD 奔向/VV 下/DT 寨/NN 我/PN 哭/VV 而/MSP 歸/VV 爲/LB 外公/NN 掌責/VV 乃/AD 獨/AD 寢/VV 遲/AD 明/AD 視/VV 阿姨/NN 已/AD 睡/VV 吾/PN 側/NN 乃/AD 不知/VV 其/PN 何時/PN 歸/VV 也/SP 公/NN 曰/VV 爾/PN 外公/NN 知/VV 阿姨/NN 歸/VV 否/AD 曰/VV 吾/PN 未/AD 之/MSP 知/VV 公/NN 曰/VV 阿/SP 姨/NN 歸/VV 時/LC 有人/PN 同行/VV 否/AD 曰/VV 無/VE 之/SP 公/NN 曰/VV 若/CS 姨女/NN 流/VV 烏/NN 敢/VV\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Print top K chunks per group (Translated vs Original) ---\n",
    "K = 5\n",
    "\n",
    "# If you know the exact group names, set them here; else auto-detect:\n",
    "target_groups = [\"Translated Novels\", \"Original Novels\"]\n",
    "available = set(df[\"group\"].unique())\n",
    "groups_to_use = [g for g in target_groups if g in available] or list(available)\n",
    "\n",
    "for g in groups_to_use:\n",
    "    sub = df[df[\"group\"] == g]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    topk = sub.nlargest(K, \"function_word_rel\", keep=\"all\").head(K)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Top {len(topk)} chunks in group: {g}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    for _, row in topk.iterrows():\n",
    "        print_chunk(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf788b6c-4fea-46ef-9b83-2de8545e80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 組別: Original Novels | 檔名: Yuan hai ling guang_chunk52 | 分塊ID: 52 ===\n",
      "PN 代詞總數: 13\n",
      "各代詞計數（token:次數）: 吾:4  爾:3  其:3  我:1  有人:1  之:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Original Novels | 檔名: Jinghua bi xue lu_chunk175 | 分塊ID: 175 ===\n",
      "PN 代詞總數: 12\n",
      "各代詞計數（token:次數）: 其:5  汝:3  爾:2  人家:1  吾:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Original Novels | 檔名: Jinguo Yangqiu_chunk134 | 分塊ID: 134 ===\n",
      "PN 代詞總數: 12\n",
      "各代詞計數（token:次數）: 吾:5  爾:4  汝:2  若:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Original Novels | 檔名: Yuan hai ling guang_chunk58 | 分塊ID: 58 ===\n",
      "PN 代詞總數: 12\n",
      "各代詞計數（token:次數）: 吾:4  之:2  爾:2  此:1  其:1  汝:1  餘:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Original Novels | 檔名: Yuan hai ling guang_chunk93 | 分塊ID: 93 ===\n",
      "PN 代詞總數: 12\n",
      "各代詞計數（token:次數）: 我:4  爾:2  吾:2  之:1  其:1  何時:1  有人:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Translated Novels | 檔名: Aiji jinta poushi ji_chunk478 | 分塊ID: 478 ===\n",
      "PN 代詞總數: 24\n",
      "各代詞計數（token:次數）: 汝:8  吾:4  之:4  爾:3  我:2  何:1  余:1  此:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Translated Novels | 檔名: Aiji jinta poushi ji_chunk707 | 分塊ID: 707 ===\n",
      "PN 代詞總數: 24\n",
      "各代詞計數（token:次數）: 汝:9  吾:5  其:2  我:2  之:2  誰:2  余:1  此:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Translated Novels | 檔名: Aisilan qingxia zhuan_chunk465 | 分塊ID: 465 ===\n",
      "PN 代詞總數: 24\n",
      "各代詞計數（token:次數）: 吾:12  汝:5  爾:3  君:1  其:1  之:1  何:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Translated Novels | 檔名: Sanqian nian yan shi ji_chunk441 | 分塊ID: 441 ===\n",
      "PN 代詞總數: 24\n",
      "各代詞計數（token:次數）: 汝:12  我:5  吾:4  爾:1  之:1  余:1\n",
      "--------------------------------------------------------------------------------\n",
      "=== 組別: Translated Novels | 檔名: Aiji jinta poushi ji_chunk139 | 分塊ID: 139 ===\n",
      "PN 代詞總數: 23\n",
      "各代詞計數（token:次數）: 爾:9  吾:4  之:3  我:2  其:2  汝:1  此:1  余:1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "K = 5\n",
    "\n",
    "# --- 只擷取 POS=PN 的代詞 ---\n",
    "def extract_PN(tokens, tags):\n",
    "    return [tok for tok, tag in zip(tokens, tags) if tag == \"PN\"]\n",
    "\n",
    "# 若先前已建立 top_pron_chunks（每組前5個 chunk），直接使用；\n",
    "# 否則依 POS=PN 計算每組前5個 chunk。\n",
    "if 'top_pron_chunks' in globals():\n",
    "    top5 = top_pron_chunks.copy()\n",
    "else:\n",
    "    if 'pron_PN_tokens' not in df.columns:\n",
    "        df['pron_PN_tokens'] = df.apply(lambda r: extract_PN(r['tokens'], r['pos']), axis=1)\n",
    "    df['pron_PN_total'] = df['pron_PN_tokens'].apply(len)\n",
    "\n",
    "    # 依每組（如 \"Translated Novels\", \"Original Novels\"）取代詞最多的前 K 個 chunk\n",
    "    top5 = (\n",
    "        df.sort_values(['group', 'pron_PN_total'], ascending=[True, False])\n",
    "          .groupby('group', group_keys=False)\n",
    "          .head(K)\n",
    "          .copy()\n",
    "    )\n",
    "\n",
    "# 確保有 PN 清單與總數欄位（以防 top5 來自他處）\n",
    "if 'pron_PN_tokens' not in top5.columns:\n",
    "    top5['pron_PN_tokens'] = top5.apply(lambda r: extract_PN(r['tokens'], r['pos']), axis=1)\n",
    "if 'pron_PN_total' not in top5.columns:\n",
    "    top5['pron_PN_total'] = top5['pron_PN_tokens'].apply(len)\n",
    "\n",
    "# --- 逐 chunk 統計「每個 PN 代詞 token 的計數」並列印 ---\n",
    "def print_pn_counts_for_chunk(row):\n",
    "    gid = row.get('chunk_id')\n",
    "    print(f\"=== 組別: {row['group']} | 檔名: {row['filename']} | 分塊ID: {gid} ===\")\n",
    "    c = Counter(row['pron_PN_tokens'])\n",
    "    print(f\"PN 代詞總數: {row['pron_PN_total']}\")\n",
    "    if c:\n",
    "        pairs = \"  \".join(f\"{tok}:{cnt}\" for tok, cnt in c.most_common())\n",
    "        print(\"各代詞計數（token:次數）:\", pairs)\n",
    "    else:\n",
    "        print(\"（本分塊無 PN 代詞）\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "for _, r in top5.iterrows():\n",
    "    print_pn_counts_for_chunk(r)\n",
    "\n",
    "# --- 同步建一個長表，便於後續分析或存檔 ---\n",
    "rows = []\n",
    "for _, r in top5.iterrows():\n",
    "    c = Counter(r['pron_PN_tokens'])\n",
    "    if c:\n",
    "        for tok, cnt in c.items():\n",
    "            rows.append({\n",
    "                \"group\": r[\"group\"],\n",
    "                \"filename\": r[\"filename\"],\n",
    "                \"chunk_id\": r.get(\"chunk_id\"),\n",
    "                \"pn_token\": tok,\n",
    "                \"count\": cnt\n",
    "            })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"group\": r[\"group\"],\n",
    "            \"filename\": r[\"filename\"],\n",
    "            \"chunk_id\": r.get(\"chunk_id\"),\n",
    "            \"pn_token\": None,\n",
    "            \"count\": 0\n",
    "        })\n",
    "\n",
    "pn_counts_per_chunk = pd.DataFrame(rows).sort_values(\n",
    "    [\"group\", \"filename\", \"chunk_id\", \"count\"], ascending=[True, True, True, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 如需存檔：\n",
    "# pn_counts_per_chunk.to_csv(\"top5_chunks_PN_代詞逐塊計數.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bd1ff-2288-42ae-a671-072868be02b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
