{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab64605b-e174-4b93-ba59-a155810ecdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"all_Lin_tokens.json\", encoding='utf-8') as f:\n",
    "    all_text_tokens = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e58f720-d598-49b4-8782-12443906fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(all_text_tokens.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed6c890-b548-4ef6-8c85-e61dd4312429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 文件夹路径\n",
    "folder_path = 'Lin'\n",
    "\n",
    "file_folder_mapping = {}\n",
    "file_list=[]\n",
    "\n",
    "# 遍历文件夹中的文件和文件夹\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file_name in files:\n",
    "        # 只处理.txt文件\n",
    "        if file_name.endswith('.txt'):\n",
    "            # 使用文件名作为键，文件夹名作为值\n",
    "            file_folder_mapping[file_name[:-4]] = os.path.basename(root)\n",
    "            file_list.append(file_name[:-4])\n",
    "\n",
    "print(len(file_folder_mapping))\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14d481b-9696-42b8-addd-4d2e4416160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. 之          — 45384 times\n",
      "  2. 曰          — 19630 times\n",
      "  3. 吾          — 18577 times\n",
      "  4. 以          — 16643 times\n",
      "  5. 而          — 16603 times\n",
      "  6. 余          — 16392 times\n",
      "  7. 此          — 15858 times\n",
      "  8. 其          — 15287 times\n",
      "  9. 人          — 13703 times\n",
      " 10. 不          — 13471 times\n",
      " 11. 亦          — 12235 times\n",
      " 12. 者          — 10179 times\n",
      " 13. 中          — 9894 times\n",
      " 14. 有          — 9547 times\n",
      " 15. 爲          — 9009 times\n",
      " 16. 言          — 8596 times\n",
      " 17. 乃          — 8506 times\n",
      " 18. 我          — 8323 times\n",
      " 19. 於          — 8223 times\n",
      " 20. 一          — 7897 times\n",
      " 21. 至          — 7744 times\n",
      " 22. 為          — 7704 times\n",
      " 23. 爾          — 7586 times\n",
      " 24. 矣          — 7327 times\n",
      " 25. 已          — 7304 times\n",
      " 26. 則          — 7131 times\n",
      " 27. 見          — 7104 times\n",
      " 28. 也          — 6963 times\n",
      " 29. 所          — 6719 times\n",
      " 30. 無          — 6690 times\n",
      " 31. 卽          — 6618 times\n",
      " 32. 汝          — 6404 times\n",
      " 33. 且          — 6154 times\n",
      " 34. 時          — 6112 times\n",
      " 35. 然          — 5814 times\n",
      " 36. 與          — 5361 times\n",
      " 37. 得          — 5092 times\n",
      " 38. 可          — 5003 times\n",
      " 39. 自          — 5001 times\n",
      " 40. 及          — 4650 times\n",
      " 41. 行          — 4526 times\n",
      " 42. 如          — 4505 times\n",
      " 43. 女          — 4196 times\n",
      " 44. 出          — 4193 times\n",
      " 45. 二          — 4192 times\n",
      " 46. 死          — 4187 times\n",
      " 47. 大          — 4144 times\n",
      " 48. 能          — 4118 times\n",
      " 49. 今          — 4083 times\n",
      " 50. 彼          — 4064 times\n",
      " 51. 上          — 3950 times\n",
      " 52. 知          — 3869 times\n",
      " 53. 復          — 3869 times\n",
      " 54. 何          — 3809 times\n",
      " 55. 必          — 3788 times\n",
      " 56. 尚          — 3735 times\n",
      " 57. 聞          — 3621 times\n",
      " 58. 事          — 3554 times\n",
      " 59. 語          — 3541 times\n",
      " 60. 後          — 3478 times\n",
      " 61. 非          — 3470 times\n",
      " 62. 下          — 3467 times\n",
      " 63. 不能         — 3336 times\n",
      " 64. 入          — 3251 times\n",
      " 65. 在          — 3131 times\n",
      " 66. 當          — 3128 times\n",
      " 67. 遂          — 3124 times\n",
      " 68. 將          — 3058 times\n",
      " 69. 欲          — 2982 times\n",
      " 70. 日          — 2957 times\n",
      " 71. 是          — 2950 times\n",
      " 72. 來          — 2879 times\n",
      " 73. 王          — 2861 times\n",
      " 74. 似          — 2787 times\n",
      " 75. 君          — 2769 times\n",
      " 76. 又          — 2759 times\n",
      " 77. 立          — 2735 times\n",
      " 78. 果          — 2637 times\n",
      " 79. 歸          — 2635 times\n",
      " 80. 舟          — 2599 times\n",
      " 81. 狀          — 2512 times\n",
      " 82. 皆          — 2490 times\n",
      " 83. 勿          — 2478 times\n",
      " 84. 衆          — 2457 times\n",
      " 85. 未          — 2455 times\n",
      " 86. 心          — 2424 times\n",
      " 87. 前          — 2421 times\n",
      " 88. 令          — 2400 times\n",
      " 89. 去          — 2387 times\n",
      " 90. 作          — 2375 times\n",
      " 91. 卷          — 2362 times\n",
      " 92. 更          — 2341 times\n",
      " 93. 耶          — 2320 times\n",
      " 94. 力          — 2297 times\n",
      " 95. 若          — 2295 times\n",
      " 96. 餘          — 2247 times\n",
      " 97. 此時         — 2238 times\n",
      " 98. 但          — 2235 times\n",
      " 99. 甚          — 2203 times\n",
      "100. 故          — 2196 times\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "# === Assume `texts` is a list of token lists ===\n",
    "# Example: texts = [['我', '是'], ['他', '也', '是'], ...]\n",
    "\n",
    "# === Step 0: Define stoplist ===\n",
    "stoplist = set([\n",
    "    \n",
    "])\n",
    "\n",
    "# Step 1: Document frequency counter\n",
    "doc_freq = Counter()\n",
    "for tokens in texts:\n",
    "    doc_freq.update(set(tokens))\n",
    "\n",
    "# Step 2: Global term frequency\n",
    "flat_tokens = list(chain.from_iterable(texts))\n",
    "word_counts = Counter(flat_tokens)\n",
    "\n",
    "# Step 3: Filter by document frequency and stoplist\n",
    "filtered_word_counts = {\n",
    "    word: freq for word, freq in word_counts.items()\n",
    "    if doc_freq[word] >= 1 and word not in stoplist\n",
    "}\n",
    "\n",
    "# Step 4: Get top-N words for vocab\n",
    "N = 100\n",
    "top_N = sorted(filtered_word_counts.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "vocab = [word for word, _ in top_N]\n",
    "\n",
    "# Step 5: Print results\n",
    "for i, (word, freq) in enumerate(top_N, 1):\n",
    "    print(f\"{i:>3}. {word:<10} — {freq} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52722a20-4f57-4fce-994a-e3cd3443f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize matrix rows\n",
    "vectors = []\n",
    "\n",
    "for tokens in texts:\n",
    "    token_count = len(tokens)\n",
    "    counter = Counter(tokens)\n",
    "    vector = [counter[word] / token_count if token_count > 0 else 0 for word in vocab]\n",
    "    vectors.append(vector)\n",
    "\n",
    "# Create DataFrame\n",
    "df_vectors = pd.DataFrame(vectors, columns=vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d7e25a-a1e9-4797-8d3b-46f369beefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_meta = pd.DataFrame({\n",
    "    \"filename\": file_list,\n",
    "    \"folder\": [file_folder_mapping.get(fn, \"unknown\") for fn in file_list]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d0e1f6-3e20-4c46-a97a-09267105d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.000005)\n",
    "df_reduced = selector.fit_transform(df_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df84dd2e-b2fd-4ecd-a342-26f2b05a34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get mask of kept features\n",
    "mask = selector.get_support()\n",
    "\n",
    "# Apply to column names\n",
    "retained_features = df_vectors.columns[mask]\n",
    "\n",
    "# Optionally convert back to DataFrame\n",
    "df_reduced_df = pd.DataFrame(df_reduced, columns=retained_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3cc7d8-ed30-482d-ac11-7e65a0bbb8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 82 features were removed (low variance ≤ 0.00001):\n",
      "\n",
      "以\n",
      "不\n",
      "亦\n",
      "者\n",
      "中\n",
      "有\n",
      "言\n",
      "於\n",
      "一\n",
      "至\n",
      "矣\n",
      "已\n",
      "則\n",
      "見\n",
      "也\n",
      "所\n",
      "無\n",
      "且\n",
      "時\n",
      "然\n",
      "與\n",
      "得\n",
      "可\n",
      "自\n",
      "及\n",
      "行\n",
      "如\n",
      "出\n",
      "二\n",
      "死\n",
      "大\n",
      "能\n",
      "今\n",
      "彼\n",
      "上\n",
      "知\n",
      "復\n",
      "何\n",
      "必\n",
      "尚\n",
      "聞\n",
      "事\n",
      "語\n",
      "後\n",
      "非\n",
      "下\n",
      "不能\n",
      "入\n",
      "在\n",
      "當\n",
      "遂\n",
      "將\n",
      "欲\n",
      "日\n",
      "是\n",
      "來\n",
      "似\n",
      "君\n",
      "又\n",
      "立\n",
      "果\n",
      "歸\n",
      "舟\n",
      "狀\n",
      "皆\n",
      "勿\n",
      "衆\n",
      "未\n",
      "心\n",
      "前\n",
      "令\n",
      "去\n",
      "作\n",
      "卷\n",
      "更\n",
      "耶\n",
      "力\n",
      "若\n",
      "此時\n",
      "但\n",
      "甚\n",
      "故\n"
     ]
    }
   ],
   "source": [
    "# Get mask of retained features (True = kept)\n",
    "mask = selector.get_support()\n",
    "\n",
    "# All feature names from original DataFrame\n",
    "all_features = df_vectors.columns\n",
    "\n",
    "# Get removed features\n",
    "removed_features = all_features[~mask]\n",
    "\n",
    "# Print removed features\n",
    "print(f\"❌ {len(removed_features)} features were removed (low variance ≤ 0.00001):\\n\")\n",
    "for feature in removed_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459a2e27-5109-472c-8f09-57921495d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 18 features were retained (variance > 0.00001):\n",
      "\n",
      "之\n",
      "曰\n",
      "吾\n",
      "而\n",
      "余\n",
      "此\n",
      "其\n",
      "人\n",
      "爲\n",
      "乃\n",
      "我\n",
      "為\n",
      "爾\n",
      "卽\n",
      "汝\n",
      "女\n",
      "王\n",
      "餘\n"
     ]
    }
   ],
   "source": [
    "# Get mask of retained features (True = kept)\n",
    "mask = selector.get_support()\n",
    "\n",
    "# Get retained feature names\n",
    "retained_features = df_vectors.columns[mask]\n",
    "\n",
    "# Print the result\n",
    "print(f\"✅ {len(retained_features)} features were retained (variance > 0.00001):\\n\")\n",
    "for feature in retained_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb373a45-17bf-4578-b734-f31ba9b6bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Optional color mapping\n",
    "genreColor = {\n",
    "    'Original Novels':\"magenta\",'Translated Novels':\"green\"\n",
    "}\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genreColor).fillna(\"gray\")\n",
    "\n",
    "# === STEP 4: Standardize and apply PCA ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# === STEP 5: Plot PCA (no text labels) ===\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "for genre, group in df_meta.groupby(\"folder\"):\n",
    "    indices = group.index\n",
    "    plt.scatter(\n",
    "        X_pca[indices, 0],\n",
    "        X_pca[indices, 1],\n",
    "        label=genre,\n",
    "        color=genreColor.get(genre, \"gray\"),\n",
    "        alpha=0.7,\n",
    "        s=10\n",
    "    )\n",
    "\n",
    "plt.title(\"PCA of Full Texts Colored by Genre\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "\n",
    "plt.legend(title=\"Datasets\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"pca_filtered300_varianceadjusted3.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca24ba-74a1-4c26-95a9-77d8a92bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# === 假设你已有 df_vectors, df_meta，其中 df_meta 包含 'filename' 和 'folder' ===\n",
    "\n",
    "# 1. Genre → color 映射\n",
    "genre_color_map = {\n",
    "    'Original Novels':\"magenta\",'Translated Novels':\"green\"\n",
    "}\n",
    "df_meta = df_meta.copy()\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genre_color_map).fillna(\"gray\")\n",
    "label_to_color = df_meta.set_index(\"filename\")[\"color\"].to_dict()\n",
    "\n",
    "# 2. 层次聚类\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# 3. 获取标签顺序（用于后续配色）\n",
    "labels = df_meta[\"filename\"].tolist()\n",
    "label_colors = [label_to_color.get(name, \"gray\") for name in labels]\n",
    "\n",
    "# 4. 画 dendrogram（带标签，每个标签颜色不同）\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    orientation=\"left\",\n",
    "    labels=labels,\n",
    "    leaf_font_size=8,\n",
    "    color_threshold=0.5 * np.max(Z[:, 2]),\n",
    "    above_threshold_color='black',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# 5. 根据叶子顺序设置每个标签颜色\n",
    "xlbls = ax.get_yticklabels()\n",
    "leaf_order = dendro[\"leaves\"]\n",
    "for lbl, i in zip(xlbls, leaf_order):\n",
    "    filename = df_meta.iloc[i][\"filename\"]\n",
    "    lbl.set_color(label_to_color.get(filename, \"gray\"))\n",
    "\n",
    "# 6. 美化图表\n",
    "ax.set_title(\"Hierarchical Clustering of Texts by Different Translators\", fontsize=14)\n",
    "ax.set_xlabel(\"Distance\")\n",
    "ax.set_ylabel(\"Texts\")\n",
    "\n",
    "# 图例\n",
    "legend_patches = [mpatches.Patch(color=c, label=g) for g, c in genre_color_map.items()]\n",
    "# 图例（更新位置）\n",
    "# 图例（放在整个图像的左上角，而非仅限于坐标轴）\n",
    "fig.legend(\n",
    "    handles=legend_patches,\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(0.05, 0.95),  # 相对于整张图的左上角\n",
    "    frameon=True,\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.75)\n",
    "\n",
    "fig.savefig(\"HCA_labels_colored_Lin.pdf\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf9452-a882-4999-bdc8-a7e65d6e5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# === 假设你已有 df_vectors 和 df_meta，其中 df_meta 包含 'filename' 和 'folder' ===\n",
    "\n",
    "# 1. Genre → color 映射\n",
    "genre_color_map = {\n",
    "    'Original Novels':\"magenta\",'Translated Novels':\"green\"\n",
    "}\n",
    "df_meta = df_meta.copy()\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genre_color_map).fillna(\"gray\")\n",
    "label_to_color = df_meta.set_index(\"filename\")[\"color\"].to_dict()\n",
    "\n",
    "# 2. 层次聚类\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# 3. 画 dendrogram（不显示标签）\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "max_d = np.max(Z[:, 2])\n",
    "cutoff = 0.5 * max_d\n",
    "\n",
    "\n",
    "\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    orientation=\"left\",\n",
    "    labels=None,\n",
    "    no_labels=True,\n",
    "    color_threshold=cutoff,       # ← sets colored clusters\n",
    "    above_threshold_color='black',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "\n",
    "for line in ax.get_lines():\n",
    "    line.set_linewidth(0.8)  # reduce visual weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. 获取叶子节点顺序与对应颜色\n",
    "leaf_order = dendro[\"leaves\"]\n",
    "leaf_filenames = df_meta.iloc[leaf_order][\"filename\"].tolist()\n",
    "leaf_colors = [label_to_color.get(name, \"gray\") for name in leaf_filenames]\n",
    "\n",
    "# 5. 提取真实的 y 坐标\n",
    "icoord = dendro[\"icoord\"]\n",
    "dcoord = dendro[\"dcoord\"]\n",
    "\n",
    "leaf_y_coords = []\n",
    "seen = 0\n",
    "for xs, ys in zip(icoord, dcoord):\n",
    "    for i, y_val in enumerate(ys):\n",
    "        if y_val == 0.0:  # 是叶节点\n",
    "            y_pos = xs[i]\n",
    "            leaf_y_coords.append(y_pos)\n",
    "            seen += 1\n",
    "            if seen >= len(leaf_order):\n",
    "                break\n",
    "    if seen >= len(leaf_order):\n",
    "        break\n",
    "\n",
    "# 6. 创建 inset_axes 画色块\n",
    "inset_ax = inset_axes(\n",
    "    ax, width=\"2.5%\", height=\"100%\", loc=\"right\",\n",
    "    bbox_to_anchor=(0.03, 0, 1, 1),  # 向右稍偏，避免重叠\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0\n",
    ")\n",
    "inset_ax.set_xlim(0, 1)\n",
    "inset_ax.set_ylim(ax.get_ylim())\n",
    "inset_ax.axis('off')\n",
    "\n",
    "# 7. 绘制彩色方块（确保无 overlap）\n",
    "block_height = 6.55\n",
    "for y, color in zip(leaf_y_coords, leaf_colors):\n",
    "    inset_ax.add_patch(plt.Rectangle(\n",
    "        (0, y - block_height / 2),\n",
    "        width=1,\n",
    "        height=block_height,\n",
    "        facecolor=color,\n",
    "        edgecolor='black',\n",
    "        linewidth=0.2\n",
    "    ))\n",
    "\n",
    "# 8. 美化与图例\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.set_title(\"Hierarchical Clustering with Genre Color Blocks\", fontsize=14)\n",
    "ax.set_xlabel(\"Distance\")\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=c, label=g) for g, c in genre_color_map.items()]\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.82)\n",
    "plt.show()\n",
    "#fig.savefig(\"HCA300_varianceadjusted3.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b37dbf-08ec-4a99-aefd-ac4e6414ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypinyin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3546b2-ddd3-4d2a-af78-09cf1c0a9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from pypinyin import lazy_pinyin\n",
    "import os\n",
    "\n",
    "# === 1. Prepare metadata ===\n",
    "\n",
    "# Example genre-to-color mapping\n",
    "genre_color_map = {\n",
    "    'Lin Shu': \"magenta\",\n",
    "    'Bao Tianxiao': \"green\",\n",
    "    'Xu Nianci': 'blue',\n",
    "    'Wu Tao': 'black',\n",
    "    'Tang Hongfu': \"red\",\n",
    "    'Zhou Guisheng': \"purple\",\n",
    "    'Sun Yuxiu': \"aqua\"\n",
    "}\n",
    "\n",
    "# Copy and process metadata\n",
    "df_meta = df_meta.copy().reset_index(drop=True)\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genre_color_map).fillna(\"gray\")\n",
    "\n",
    "# Optional: remove file extension before converting to pinyin\n",
    "df_meta[\"pinyin_label\"] = df_meta[\"filename\"].apply(\n",
    "    lambda x: ' '.join(lazy_pinyin(os.path.splitext(x)[0])).capitalize()\n",
    ")\n",
    "\n",
    "# Mapping from pinyin label → color\n",
    "pinyin_to_color = df_meta.set_index(\"pinyin_label\")[\"color\"].to_dict()\n",
    "\n",
    "# === 2. Standardize and cluster vectors ===\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)  # df_reduced should be your feature matrix\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# === 3. Get leaf order and corresponding labels ===\n",
    "\n",
    "leaf_order = dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "labels_in_order = df_meta.iloc[leaf_order][\"pinyin_label\"].tolist()\n",
    "\n",
    "# === 4. Plot dendrogram ===\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "max_d = np.max(Z[:, 2])\n",
    "cutoff = 0.5 * max_d\n",
    "\n",
    "dendro = dendrogram(\n",
    "    Z,\n",
    "    orientation=\"left\",\n",
    "    labels=labels_in_order,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=cutoff,\n",
    "    above_threshold_color='black',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# === 5. Set label colors based on genre ===\n",
    "\n",
    "for label in ax.get_yticklabels():\n",
    "    pinyin = label.get_text()\n",
    "    label.set_color(pinyin_to_color.get(pinyin, \"gray\"))\n",
    "\n",
    "# === 6. Legend and formatting ===\n",
    "\n",
    "ax.set_title(\"Hierarchical Clustering with Pinyin Labels Colored by Genre\", fontsize=14)\n",
    "ax.set_xlabel(\"Distance\")\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=c, label=g) for g, c in genre_color_map.items()]\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\", title=\"Translator/Author\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# === 7. Save or show plot ===\n",
    "\n",
    "SAVE_PDF = True\n",
    "if SAVE_PDF:\n",
    "    fig.savefig(\"hca_pinyin_colored.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648c4e7-730e-42c9-8d08-f48221dd6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_meta) == df_reduced.shape[0]\n",
    "df_meta = df_meta.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e6517-9d0a-4082-83be-af214ead99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 获取叶子顺序（leaf_order 是索引）\n",
    "leaf_order = dendro[\"leaves\"]\n",
    "leaf_filenames = df_meta.iloc[leaf_order][\"filename\"].tolist()\n",
    "leaf_colors = [label_to_color.get(name, \"gray\") for name in leaf_filenames]\n",
    "\n",
    "# 2. 获取各颜色在叶子顺序中的位置索引\n",
    "positions = {\"magenta\": [], \"green\": [], \"blue\": []}\n",
    "\n",
    "for i, color in enumerate(leaf_colors):\n",
    "    if color in positions:\n",
    "        positions[color].append(i)\n",
    "\n",
    "# 3. 计算各颜色的中位数位置\n",
    "medians = {color: np.median(pos) for color, pos in positions.items()}\n",
    "print(\"中位位置:\", medians)\n",
    "\n",
    "# 4. 可视化分布\n",
    "plt.boxplot([positions[\"magenta\"], positions[\"green\"], positions[\"blue\"]],\n",
    "            labels=[\"Original (magenta)\", \"Translated (green)\", \"Martial Arts (blue)\"])\n",
    "plt.ylabel(\"Position in leaf order\")\n",
    "plt.title(\"Leaf Position Distribution by Genre\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29353492-ecc8-40bc-a890-e731bbed478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_vectors.columns[selector.get_support()]\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633a4ae-982f-4128-bc25-3585d9dd6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T * 50,       # scale for visualization\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85605e61-78a8-4abd-be2f-d88dbc820354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words most strongly correlated with PC1\n",
    "print(\"Top contributors to PC1:\")\n",
    "print(loadings[\"PC1\"].sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop negative contributors to PC1:\")\n",
    "print(loadings[\"PC1\"].sort_values().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea765be-6eb4-46fc-b9f6-bbfca19ebc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words most strongly correlated with PC1\n",
    "print(\"Top contributors to PC2:\")\n",
    "print(loadings[\"PC2\"].sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop negative contributors to PC2:\")\n",
    "print(loadings[\"PC2\"].sort_values().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84611b9f-573c-474c-80aa-3ca5eb34eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "\n",
    "# === Step 2: Genre to color mapping ===\n",
    "genreColor = {\n",
    "   'Original Novels':\"magenta\",'Translated Novels':\"green\"\n",
    "}\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genreColor).fillna(\"gray\")\n",
    "\n",
    "# === Step 3: Standardize selected features ===\n",
    "X_scaled = StandardScaler().fit_transform(df_reduced)\n",
    "\n",
    "# === Step 4: Apply PCA ===\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# === Step 5: Compute loadings ===\n",
    "\n",
    "# === Step 6: Select top contributors ===\n",
    "top_pc1 = loadings[\"PC1\"].abs().sort_values(ascending=False).head(25)\n",
    "top_pc2 = loadings[\"PC2\"].abs().sort_values(ascending=False).head(25)\n",
    "top_features = top_pc1.index.union(top_pc2.index)\n",
    "df_top_loadings = loadings.loc[top_features]\n",
    "\n",
    "# === Step 7: Plot PCA scatter + loadings ===\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot documents by genre (safe: use boolean mask)\n",
    "for genre, group in df_meta.groupby(\"folder\"):\n",
    "    indices = group.index\n",
    "    plt.scatter(\n",
    "        X_pca[indices, 0],\n",
    "        X_pca[indices, 1],\n",
    "        label=genre,\n",
    "        color=genreColor.get(genre, \"gray\"),\n",
    "        alpha=0.7,\n",
    "        s=10\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot loadings: arrows + text\n",
    "for word, row in df_top_loadings.iterrows():\n",
    "    x, y = row[\"PC1\"], row[\"PC2\"]\n",
    "    \n",
    "    plt.text(\n",
    "        x, y, word,\n",
    "        fontsize=12,\n",
    "        color='black',\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontname='STSong',\n",
    "        weight='bold'\n",
    "    )\n",
    "\n",
    "\n",
    "# Final layout\n",
    "plt.title(\"PCA of Texts with Top Feature Loadings (×100)\", fontsize=16)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.legend(title=\"Genre\", fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: save loadings\n",
    "df_top_loadings.to_csv(\"pca_top_loadings_variance_threshold.csv\", encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a5f4-f66d-4f0c-9113-9e6dabc61f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "\n",
    "# === Step 2: Genre to color mapping ===\n",
    "genreColor = {\n",
    "   'Original Novels':\"magenta\",'Translated Novels':\"green\"\n",
    "}\n",
    "df_meta[\"color\"] = df_meta[\"folder\"].map(genreColor).fillna(\"gray\")\n",
    "\n",
    "# === Step 3: Standardize selected features ===\n",
    "X_scaled = StandardScaler().fit_transform(df_reduced)\n",
    "\n",
    "# === Step 4: Apply PCA ===\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# === Step 5: Compute loadings ===\n",
    "\n",
    "# === Step 6: Select top contributors ===\n",
    "top_pc1 = loadings[\"PC1\"].abs().sort_values(ascending=False).head(25)\n",
    "top_pc2 = loadings[\"PC2\"].abs().sort_values(ascending=False).head(25)\n",
    "top_features = top_pc1.index.union(top_pc2.index)\n",
    "df_top_loadings = loadings.loc[top_features]\n",
    "\n",
    "# === Step 7: Plot PCA scatter + loadings ===\n",
    "plt.figure(figsize=(10, 9))\n",
    "\n",
    "# Plot documents by genre (safe: use boolean mask)\n",
    "for genre in df_meta[\"folder\"].unique():\n",
    "    mask = df_meta[\"folder\"] == genre\n",
    "    plt.scatter(\n",
    "        X_pca[mask, 0],\n",
    "        X_pca[mask, 1],\n",
    "        label=genre,\n",
    "        color=genreColor.get(genre, \"gray\"),\n",
    "        alpha=0.7,\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "# Plot axes\n",
    "plt.axhline(0, color='black', lw=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black', lw=0.5, linestyle='--')\n",
    "\n",
    "# Plot loadings: arrows + text\n",
    "for word, row in df_top_loadings.iterrows():\n",
    "    x, y = row[\"PC1\"], row[\"PC2\"]\n",
    "    plt.plot([0, x], [0, y],\n",
    "         color='gray', alpha=0.3, lw=0.8)\n",
    "\n",
    "    plt.text(\n",
    "        x, y, word,\n",
    "        fontsize=11,\n",
    "        color='black',\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontname='STSong',\n",
    "        weight='bold'\n",
    "    )\n",
    "\n",
    "\n",
    "# Final layout\n",
    "plt.title(\"PCA of Texts with Top Feature Loadings (×50)\", fontsize=16)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.legend(title=\"Genre\", fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"pca_top100_loadings300_varianceadjusted4.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.savefig(\"pca_Lin_varianceadjusted.pdf\", bbox_inches='tight', dpi=1000)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Optional: save loadings\n",
    "df_top_loadings.to_csv(\"pca_top_loadings_variance_Lin.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754422f3-003b-41c6-8e4c-ec19013e6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# === STEP 1: Walk through subfolders and collect filenames and genres ===\n",
    "data_root = \"corpus318\"  # Replace this with your actual path\n",
    "\n",
    "filenames = []\n",
    "genres = []\n",
    "\n",
    "for genre in os.listdir(data_root):\n",
    "    genre_path = os.path.join(data_root, genre)\n",
    "    if os.path.isdir(genre_path):\n",
    "        for fname in os.listdir(genre_path):\n",
    "            if fname.endswith(\".txt\"):\n",
    "                full_path = os.path.join(genre_path, fname)\n",
    "                filenames.append(full_path)\n",
    "                genres.append(genre)\n",
    "\n",
    "# === STEP 2: Create metadata DataFrame ===\n",
    "df_meta = pd.DataFrame({\n",
    "    \"filename\": filenames,\n",
    "    \"genre\": genres\n",
    "})\n",
    "\n",
    "genreColor = {\n",
    "    'original Chinese adventure fiction': \"magenta\",\n",
    "    'translated adventure fiction': \"green\",\n",
    "    'Chinese martial arts fiction': \"blue\"\n",
    "}\n",
    "df_meta[\"color\"] = df_meta[\"genre\"].map(genreColor).fillna(\"gray\")\n",
    "\n",
    "# === STEP 3: df_reduced should be created before this step ===\n",
    "# It should align with df_meta in row order\n",
    "# Example: df_reduced = some_vectorizer.fit_transform(texts)\n",
    "\n",
    "# For illustration only — replace with your real feature matrix\n",
    "# df_reduced = np.random.rand(len(filenames), 300)  # Replace with actual data\n",
    "\n",
    "# === STEP 4: Standardize and apply PCA ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "df_meta[\"PC1\"] = X_pca[:, 0]\n",
    "df_meta[\"PC2\"] = X_pca[:, 1]\n",
    "\n",
    "# === STEP 5: Plot PCA ===\n",
    "plt.figure(figsize=(7, 6))\n",
    "for genre, group in df_meta.groupby(\"genre\"):\n",
    "    indices = group.index\n",
    "    plt.scatter(\n",
    "        df_meta.loc[indices, \"PC1\"],\n",
    "        df_meta.loc[indices, \"PC2\"],\n",
    "        label=genre,\n",
    "        color=genreColor.get(genre, \"gray\"),\n",
    "        alpha=0.7,\n",
    "        s=10\n",
    "    )\n",
    "\n",
    "plt.title(\"PCA of Full Texts Colored by Genre\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.legend(title=\"Genres\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"pca_genres.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# === STEP 6: Print top and bottom texts by PCA axes ===\n",
    "top10_PC1 = df_meta.nlargest(10, \"PC1\")\n",
    "bottom10_PC1 = df_meta.nsmallest(10, \"PC1\")\n",
    "top10_PC2 = df_meta.nlargest(10, \"PC2\")\n",
    "bottom10_PC2 = df_meta.nsmallest(10, \"PC2\")\n",
    "\n",
    "print(\"Top 10 PC1:\\n\", top10_PC1[[\"filename\", \"PC1\"]])\n",
    "print(\"\\nBottom 10 PC1:\\n\", bottom10_PC1[[\"filename\", \"PC1\"]])\n",
    "print(\"\\nTop 10 PC2:\\n\", top10_PC2[[\"filename\", \"PC2\"]])\n",
    "print(\"\\nBottom 10 PC2:\\n\", bottom10_PC2[[\"filename\", \"PC2\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bd41a-5777-42da-ab03-9d9c8008f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top and bottom 10 by PC1\n",
    "top10_PC1.to_csv(\"top10_PC1.csv\", index=False, encoding='utf-8-sig')\n",
    "bottom10_PC1.to_csv(\"bottom10_PC1.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Save top and bottom 10 by PC2\n",
    "top10_PC2.to_csv(\"top10_PC2.csv\", index=False, encoding='utf-8-sig')\n",
    "bottom10_PC2.to_csv(\"bottom10_PC2.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20ce68-f943-4981-8975-47adc36cfbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d2c21-3742-4254-b21f-b86a91d65d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de68c78-a8b4-4473-bbba-4a5532042735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62b4fe-f012-4ec9-b509-74906a2d6647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
