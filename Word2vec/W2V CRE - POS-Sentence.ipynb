{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d7fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok/fine': [['å•†å“',\n",
       "   'å’Œ',\n",
       "   'æœåŠ¡',\n",
       "   'ã€‚',\n",
       "   'æ™“ç¾ç„°',\n",
       "   'æ¥åˆ°',\n",
       "   'åŒ—äº¬',\n",
       "   'ç«‹æ–¹åº­',\n",
       "   'å‚è§‚',\n",
       "   'è‡ªç„¶',\n",
       "   'è¯­ä¹‰',\n",
       "   'ç§‘æŠ€',\n",
       "   'å…¬å¸',\n",
       "   'ã€‚'],\n",
       "  ['æˆ‘', 'çˆ¸', 'æ˜¯', 'æåˆš']],\n",
       " 'pos/863': [['n',\n",
       "   'c',\n",
       "   'v',\n",
       "   'w',\n",
       "   'nh',\n",
       "   'v',\n",
       "   'ns',\n",
       "   'ni',\n",
       "   'v',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'w'],\n",
       "  ['r', 'n', 'v', 'nh']]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hanlp_restful import HanLPClient\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import glob\n",
    "\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=\"\", language='zh')\n",
    "HanLP.parse([\"å•†å“å’ŒæœåŠ¡ã€‚æ™“ç¾ç„°æ¥åˆ°åŒ—äº¬ç«‹æ–¹åº­å‚è§‚è‡ªç„¶è¯­ä¹‰ç§‘æŠ€å…¬å¸ã€‚\", \"æˆ‘çˆ¸æ˜¯æåˆš\"], tasks='pos/863')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91565693-4acf-4366-a54d-7ab91b1fd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 29 stopwords.\n",
      "ğŸ” Starting to process files...\n",
      "\n",
      "ğŸ“„ (1) Processing ä¸å¯æ€è®®.txt...\n",
      "ğŸ“Š ä¸å¯æ€è®®.txt: 30 sentences, 238 tokens kept.\n",
      "\n",
      "ğŸ“„ (2) Processing ä¸­å¤®äºšéåˆ©åŠ ä¹‹è›®åœ°æ¢é™©.txt...\n",
      "ğŸ“Š ä¸­å¤®äºšéåˆ©åŠ ä¹‹è›®åœ°æ¢é™©.txt: 199 sentences, 1688 tokens kept.\n",
      "\n",
      "ğŸ“„ (3) Processing å†’é™©å¤œè¡Œ.txt...\n",
      "ğŸ“Š å†’é™©å¤œè¡Œ.txt: 34 sentences, 293 tokens kept.\n",
      "\n",
      "ğŸ“„ (4) Processing å†’é™©å¥‡è°ˆ.txt...\n",
      "ğŸ“Š å†’é™©å¥‡è°ˆ.txt: 40 sentences, 390 tokens kept.\n",
      "\n",
      "ğŸ“„ (5) Processing å†’é™©å°è¯´å­¤åº—æ‘.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´å­¤åº—æ‘.txt: 80 sentences, 845 tokens kept.\n",
      "\n",
      "ğŸ“„ (6) Processing å†’é™©å°è¯´æŸéº¦èˆ¹é•¿èˆªæµ·è°ˆ.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´æŸéº¦èˆ¹é•¿èˆªæµ·è°ˆ.txt: 80 sentences, 837 tokens kept.\n",
      "\n",
      "ğŸ“„ (7) Processing å†’é™©å°è¯´ç«å±±å²›.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´ç«å±±å²›.txt: 75 sentences, 821 tokens kept.\n",
      "\n",
      "ğŸ“„ (8) Processing å†’é™©å°è¯´èˆªæµ·å®¶å„¿.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´èˆªæµ·å®¶å„¿.txt: 281 sentences, 2713 tokens kept.\n",
      "\n",
      "ğŸ“„ (9) Processing å†’é™©å°è¯´é‡‘æ²™çªŸ.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´é‡‘æ²™çªŸ.txt: 90 sentences, 1440 tokens kept.\n",
      "\n",
      "ğŸ“„ (10) Processing å†’é™©å°è¯´é¬¼åŸŸ.txt...\n",
      "ğŸ“Š å†’é™©å°è¯´é¬¼åŸŸ.txt: 181 sentences, 1279 tokens kept.\n",
      "\n",
      "ğŸ“„ (11) Processing å†’é™©æˆåŠŸä¹‹å•†ç•Œä¼Ÿäºº.txt...\n",
      "ğŸ“Š å†’é™©æˆåŠŸä¹‹å•†ç•Œä¼Ÿäºº.txt: 246 sentences, 2213 tokens kept.\n",
      "\n",
      "ğŸ“„ (12) Processing å†¢ä¸­æ€ª.txt...\n",
      "ğŸ“Š å†¢ä¸­æ€ª.txt: 84 sentences, 996 tokens kept.\n",
      "\n",
      "ğŸ“„ (13) Processing åŒ—å†°æ´‹å½’å®¢è°ˆ.txt...\n",
      "ğŸ“Š åŒ—å†°æ´‹å½’å®¢è°ˆ.txt: 22 sentences, 221 tokens kept.\n",
      "\n",
      "ğŸ“„ (14) Processing å—æ–æ´²è„±é™©è®°.txt...\n",
      "ğŸ“Š å—æ–æ´²è„±é™©è®°.txt: 239 sentences, 2029 tokens kept.\n",
      "\n",
      "ğŸ“„ (15) Processing å¶å…°.txt...\n",
      "ğŸ“Š å¶å…°.txt: 41 sentences, 373 tokens kept.\n",
      "\n",
      "ğŸ“„ (16) Processing å¸å®¾æ´›.txt...\n",
      "ğŸ“Š å¸å®¾æ´›.txt: 352 sentences, 3555 tokens kept.\n",
      "\n",
      "ğŸ“„ (17) Processing å›é£.txt...\n",
      "ğŸ“Š å›é£.txt: 77 sentences, 737 tokens kept.\n",
      "\n",
      "ğŸ“„ (18) Processing åœ°ä¸‹éƒ½ä¼šä¸æµ·åº•æ£®æ—.txt...\n",
      "ğŸ“Š åœ°ä¸‹éƒ½ä¼šä¸æµ·åº•æ£®æ—.txt: 57 sentences, 631 tokens kept.\n",
      "\n",
      "ğŸ“„ (19) Processing å¥³ä¸ˆå¤«.txt...\n",
      "ğŸ“Š å¥³ä¸ˆå¤«.txt: 904 sentences, 7423 tokens kept.\n",
      "\n",
      "ğŸ“„ (20) Processing å°ä¸­å.txt...\n",
      "ğŸ“Š å°ä¸­å.txt: 278 sentences, 2293 tokens kept.\n",
      "\n",
      "ğŸ“„ (21) Processing å²›å›½å½’å©šè®°.txt...\n",
      "ğŸ“Š å²›å›½å½’å©šè®°.txt: 200 sentences, 2096 tokens kept.\n",
      "\n",
      "ğŸ“„ (22) Processing å¼Ÿ.txt...\n",
      "ğŸ“Š å¼Ÿ.txt: 23 sentences, 253 tokens kept.\n",
      "\n",
      "ğŸ“„ (23) Processing æ€ªå›½æ¢é™©è®°.txt...\n",
      "ğŸ“Š æ€ªå›½æ¢é™©è®°.txt: 110 sentences, 981 tokens kept.\n",
      "\n",
      "ğŸ“„ (24) Processing æ¢é™©è‹±é›„ä¼ .txt...\n",
      "ğŸ“Š æ¢é™©è‹±é›„ä¼ .txt: 300 sentences, 2872 tokens kept.\n",
      "\n",
      "ğŸ“„ (25) Processing æ–°æ®–æ°‘åœ°ä»²å°¼å³¶.txt...\n",
      "ğŸ“Š æ–°æ®–æ°‘åœ°ä»²å°¼å³¶.txt: 111 sentences, 1019 tokens kept.\n",
      "\n",
      "ğŸ“„ (26) Processing æœˆçƒæ®–æ°‘åœ°å°èªª.txt...\n",
      "ğŸ“Š æœˆçƒæ®–æ°‘åœ°å°èªª.txt: 4121 sentences, 38113 tokens kept.\n",
      "\n",
      "ğŸ“„ (27) Processing æ ‘ç©´ä¹¦.txt...\n",
      "ğŸ“Š æ ‘ç©´ä¹¦.txt: 141 sentences, 1509 tokens kept.\n",
      "\n",
      "ğŸ“„ (28) Processing æ¯’æ´²æ¢é™©è®°.txt...\n",
      "ğŸ“Š æ¯’æ´²æ¢é™©è®°.txt: 272 sentences, 2780 tokens kept.\n",
      "\n",
      "ğŸ“„ (29) Processing æµ·ä¸Šé€¸å².txt...\n",
      "ğŸ“Š æµ·ä¸Šé€¸å².txt: 160 sentences, 1594 tokens kept.\n",
      "\n",
      "ğŸ“„ (30) Processing æµ·åº•å¥‡è°ˆ.txt...\n",
      "ğŸ“Š æµ·åº•å¥‡è°ˆ.txt: 2309 sentences, 21535 tokens kept.\n",
      "\n",
      "ğŸ“„ (31) Processing æ¸”çˆ¶å†’é™©è®° å† ä¸‡.txt...\n",
      "ğŸ“Š æ¸”çˆ¶å†’é™©è®° å† ä¸‡.txt: 32 sentences, 262 tokens kept.\n",
      "\n",
      "ğŸ“„ (32) Processing æ¸”çˆ¶å†’é™©è®° å­™æ˜å…´.txt...\n",
      "ğŸ“Š æ¸”çˆ¶å†’é™©è®° å­™æ˜å…´.txt: 18 sentences, 159 tokens kept.\n",
      "\n",
      "ğŸ“„ (33) Processing æ¸”çˆ¶å†’é™©è®° æ—è¿œéš†.txt...\n",
      "ğŸ“Š æ¸”çˆ¶å†’é™©è®° æ—è¿œéš†.txt: 9 sentences, 78 tokens kept.\n",
      "\n",
      "ğŸ“„ (34) Processing æ¸”çˆ¶å†’é™©è®° é»„è£ä»™.txt...\n",
      "ğŸ“Š æ¸”çˆ¶å†’é™©è®° é»„è£ä»™.txt: 23 sentences, 196 tokens kept.\n",
      "\n",
      "ğŸ“„ (35) Processing æ»•åŠä»™ä¼.txt...\n",
      "ğŸ“Š æ»•åŠä»™ä¼.txt: 58 sentences, 557 tokens kept.\n",
      "\n",
      "ğŸ“„ (36) Processing ç‰‡å¸†å½±.txt...\n",
      "ğŸ“Š ç‰‡å¸†å½±.txt: 117 sentences, 1262 tokens kept.\n",
      "\n",
      "ğŸ“„ (37) Processing ç‹®å­è¡€.txt...\n",
      "ğŸ“Š ç‹®å­è¡€.txt: 1011 sentences, 9856 tokens kept.\n",
      "\n",
      "ğŸ“„ (38) Processing ç‹¼å»ä½™ç”Ÿè®°.txt...\n",
      "ğŸ“Š ç‹¼å»ä½™ç”Ÿè®°.txt: 63 sentences, 736 tokens kept.\n",
      "\n",
      "ğŸ“„ (39) Processing ç™¾æ–¯ç¬ƒæ®–æ°‘æ”¿ç­–è®°.txt...\n",
      "ğŸ“Š ç™¾æ–¯ç¬ƒæ®–æ°‘æ”¿ç­–è®°.txt: 193 sentences, 1870 tokens kept.\n",
      "\n",
      "ğŸ“„ (40) Processing ç›—ç›—.txt...\n",
      "ğŸ“Š ç›—ç›—.txt: 10 sentences, 88 tokens kept.\n",
      "\n",
      "ğŸ“„ (41) Processing ç ´èˆŸé‡é™©è®°.txt...\n",
      "ğŸ“Š ç ´èˆŸé‡é™©è®°.txt: 42 sentences, 420 tokens kept.\n",
      "\n",
      "ğŸ“„ (42) Processing ç£Šè½å£«å®˜.txt...\n",
      "ğŸ“Š ç£Šè½å£«å®˜.txt: 101 sentences, 1037 tokens kept.\n",
      "\n",
      "ğŸ“„ (43) Processing ç©´å±….txt...\n",
      "ğŸ“Š ç©´å±….txt: 34 sentences, 394 tokens kept.\n",
      "\n",
      "ğŸ“„ (44) Processing ç©ºä¸­å²›.txt...\n",
      "ğŸ“Š ç©ºä¸­å²›.txt: 42 sentences, 457 tokens kept.\n",
      "\n",
      "ğŸ“„ (45) Processing çºªå®å†’é™©å°è¯´æ´åº­æ¹–.txt...\n",
      "ğŸ“Š çºªå®å†’é™©å°è¯´æ´åº­æ¹–.txt: 31 sentences, 376 tokens kept.\n",
      "\n",
      "ğŸ“„ (46) Processing èˆªæµ·å¥‡é‡.txt...\n",
      "ğŸ“Š èˆªæµ·å¥‡é‡.txt: 328 sentences, 2357 tokens kept.\n",
      "\n",
      "ğŸ“„ (47) Processing èˆªèˆ¹ç›—.txt...\n",
      "ğŸ“Š èˆªèˆ¹ç›—.txt: 127 sentences, 1284 tokens kept.\n",
      "\n",
      "ğŸ“„ (48) Processing è‹±ç”·å­å¥³è®°.txt...\n",
      "ğŸ“Š è‹±ç”·å­å¥³è®°.txt: 204 sentences, 1676 tokens kept.\n",
      "\n",
      "ğŸ“„ (49) Processing è’å²›å­¤è¸ª.txt...\n",
      "ğŸ“Š è’å²›å­¤è¸ª.txt: 47 sentences, 494 tokens kept.\n",
      "\n",
      "ğŸ“„ (50) Processing è’å²›æ¼‚æ³Šè®°.txt...\n",
      "ğŸ“Š è’å²›æ¼‚æ³Šè®°.txt: 117 sentences, 924 tokens kept.\n",
      "\n",
      "ğŸ“„ (51) Processing è›‡è™æ–—.txt...\n",
      "ğŸ“Š è›‡è™æ–—.txt: 85 sentences, 898 tokens kept.\n",
      "\n",
      "ğŸ“„ (52) Processing è®°äº‹å°è¯´è™å£ä½™ç”Ÿè®°.txt...\n",
      "ğŸ“Š è®°äº‹å°è¯´è™å£ä½™ç”Ÿè®°.txt: 63 sentences, 613 tokens kept.\n",
      "\n",
      "ğŸ“„ (53) Processing é™†æ²»æ–¯å—ææ¢é™©äº‹.txt...\n",
      "ğŸ“Š é™†æ²»æ–¯å—ææ¢é™©äº‹.txt: 25 sentences, 263 tokens kept.\n",
      "\n",
      "ğŸ“„ (54) Processing é’é“œé’±.txt...\n",
      "ğŸ“Š é’é“œé’±.txt: 168 sentences, 1771 tokens kept.\n",
      "\n",
      "ğŸ“„ (55) Processing éæ´²æ¢é™©è®°.txt...\n",
      "ğŸ“Š éæ´²æ¢é™©è®°.txt: 51 sentences, 448 tokens kept.\n",
      "\n",
      "ğŸ“„ (56) Processing é©å‘½å…š.txt...\n",
      "ğŸ“Š é©å‘½å…š.txt: 3096 sentences, 32412 tokens kept.\n",
      "\n",
      "ğŸ“„ (57) Processing é©®è½®å¾¡é£è®°.txt...\n",
      "ğŸ“Š é©®è½®å¾¡é£è®°.txt: 94 sentences, 853 tokens kept.\n",
      "\n",
      "ğŸ“„ (58) Processing é­”çªŸ.txt...\n",
      "ğŸ“Š é­”çªŸ.txt: 92 sentences, 856 tokens kept.\n",
      "\n",
      "ğŸ“„ (59) Processing é±¼ä¸½æ°´å†é™©è®°.txt...\n",
      "ğŸ“Š é±¼ä¸½æ°´å†é™©è®°.txt: 208 sentences, 1926 tokens kept.\n",
      "\n",
      "ğŸ“„ (60) Processing é¸­è›‹å²›æ¢é™©è®°.txt...\n",
      "ğŸ“Š é¸­è›‹å²›æ¢é™©è®°.txt: 81 sentences, 792 tokens kept.\n",
      "\n",
      "âœ… Token filtering complete.\n",
      "ğŸ“ˆ Total sentences: 17707\n",
      "ğŸ“ˆ Total tokens: 169082\n",
      "âœ… Filtered sentences saved to 'filtered_sentences2.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# === Setup ===\n",
    "root = \"dataclean3\"\n",
    "output_model_path = \"filtered_word2vec_pS.model\"\n",
    "query_timestamps = []\n",
    "texts = []\n",
    "file_stats = dict()\n",
    "\n",
    "# Load stopwords\n",
    "stopword_file = 'stopwords.txt'\n",
    "\n",
    "with open(stopword_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "print(f\"âœ… Loaded {len(stopwords)} stopwords.\")\n",
    "\n",
    "valid_pos_tags = {'n', 'v', 'a', 'ns'}\n",
    "\n",
    "MAX_BATCH_SIZE = 200\n",
    "MAX_CALLS_PER_MINUTE = 70\n",
    "TIME_WINDOW = 60\n",
    "\n",
    "# === Helpers ===\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿâ€ã€])', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def batch_sentences(sentences, batch_size=200):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i:i+batch_size]\n",
    "\n",
    "def respect_rate_limit():\n",
    "    if len(query_timestamps) >= MAX_CALLS_PER_MINUTE:\n",
    "        gap = time.time() - query_timestamps[-MAX_CALLS_PER_MINUTE]\n",
    "        if gap < TIME_WINDOW:\n",
    "            sleep_time = TIME_WINDOW - gap\n",
    "            print(f\"\\tSleeping {sleep_time:.2f} seconds to respect rate limit...\")\n",
    "            time.sleep(sleep_time)\n",
    "    if len(query_timestamps) > MAX_CALLS_PER_MINUTE:\n",
    "        del query_timestamps[:-MAX_CALLS_PER_MINUTE]\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def filter_tokens(data):\n",
    "    tokens = flatten(data['tok/fine'])\n",
    "    pos_tags = flatten(data['pos/863'])\n",
    "\n",
    "    selected_tokens = []\n",
    "    for token, pos in zip(tokens, pos_tags):\n",
    "        if (pos in valid_pos_tags) and (token not in stopwords):\n",
    "            selected_tokens.append(token)\n",
    "\n",
    "    return selected_tokens\n",
    "\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        query_timestamps.append(time.time())\n",
    "        data = HanLP.parse(batch, tasks=['tok/fine', 'pos/863'])\n",
    "\n",
    "        batch_filtered = []\n",
    "\n",
    "        for sentence_idx in range(len(batch)):\n",
    "            sentence_data = {\n",
    "                'tok/fine': [data['tok/fine'][sentence_idx]],\n",
    "                'pos/863': [data['pos/863'][sentence_idx]]\n",
    "            }\n",
    "            filtered_tokens = filter_tokens(sentence_data)\n",
    "            if filtered_tokens:\n",
    "                batch_filtered.append(filtered_tokens)\n",
    "\n",
    "        return batch_filtered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â— Error processing batch: {e}\")\n",
    "        return []\n",
    "\n",
    "# === Main Loop ===\n",
    "\n",
    "total_tokens_all_files = 0\n",
    "total_sentences_all_files = 0\n",
    "\n",
    "print(\"ğŸ” Starting to process files...\")\n",
    "\n",
    "for file_idx, file_name in enumerate(os.listdir(root), 1):\n",
    "    if not file_name.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“„ ({file_idx}) Processing {file_name}...\")\n",
    "    file_path = os.path.join(root, file_name)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    file_sentences = []\n",
    "    file_token_count = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(batch_sentences(sentences, batch_size=MAX_BATCH_SIZE)):\n",
    "        respect_rate_limit()\n",
    "        filtered_batch = process_batch(batch)\n",
    "\n",
    "        if filtered_batch:\n",
    "            file_sentences.extend(filtered_batch)\n",
    "\n",
    "    texts.extend(file_sentences)\n",
    "    total_sentences_all_files += len(file_sentences)\n",
    "    file_tokens = sum(len(sentence) for sentence in file_sentences)\n",
    "    total_tokens_all_files += file_tokens\n",
    "\n",
    "    print(f\"ğŸ“Š {file_name}: {len(file_sentences)} sentences, {file_tokens} tokens kept.\")\n",
    "\n",
    "print(\"\\nâœ… Token filtering complete.\")\n",
    "print(f\"ğŸ“ˆ Total sentences: {total_sentences_all_files}\")\n",
    "print(f\"ğŸ“ˆ Total tokens: {total_tokens_all_files}\")\n",
    "\n",
    "# === Save filtered sentences (optional) ===\n",
    "with open(\"filtered_sentences2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Filtered sentences saved to 'filtered_sentences2.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825566d-c41f-498b-8004-dc1f18aa7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered_sentences1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8113dd2-fbd9-4308-9858-9d05638bcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  # è¾“å‡ºæ—¥å¿—ä¿¡æ¯\n",
    "# sentences = word2vec.Text8Corpus('texts')  # å°†è¯­æ–™ä¿å­˜åœ¨sentenceä¸­\n",
    "model = word2vec.Word2Vec(texts, sg=1, vector_size=100,  window=5,  min_count=2,  negative=0, sample=0.001, hs=1, workers=4, epochs=20)  # ç”Ÿæˆè¯å‘é‡ç©ºé—´æ¨¡å‹\n",
    "model.save('cre_all_word2vec1cS.model')  # ä¿å­˜æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe8f7aa-7f1d-48d8-b653-cf728f158348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:56:57,549 : INFO : collecting all words and their counts\n",
      "2025-04-21 23:56:57,550 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-21 23:56:57,574 : INFO : PROGRESS: at sentence #10000, processed 92828 words, keeping 15917 word types\n",
      "2025-04-21 23:56:57,589 : INFO : collected 22484 word types from a corpus of 169082 raw words and 17707 sentences\n",
      "2025-04-21 23:56:57,589 : INFO : Creating a fresh vocabulary\n",
      "2025-04-21 23:56:57,638 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11221 unique words (49.91% of original 22484, drops 11263)', 'datetime': '2025-04-21T23:56:57.638369', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,638 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 157819 word corpus (93.34% of original 169082, drops 11263)', 'datetime': '2025-04-21T23:56:57.638369', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,743 : INFO : deleting the raw counts dictionary of 22484 items\n",
      "2025-04-21 23:56:57,744 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2025-04-21 23:56:57,745 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 145237.87325628963 word corpus (92.0%% of prior 157819)', 'datetime': '2025-04-21T23:56:57.745427', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,751 : INFO : constructing a huffman tree from 11221 words\n",
      "2025-04-21 23:56:58,134 : INFO : built huffman tree with maximum node depth 16\n",
      "2025-04-21 23:56:58,147 : INFO : estimated required memory for 11221 words and 100 dimensions: 16831500 bytes\n",
      "2025-04-21 23:56:58,148 : INFO : resetting layer weights\n",
      "2025-04-21 23:56:58,154 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-21T23:56:58.154549', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2025-04-21 23:56:58,154 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 11221 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=0 window=5 shrink_windows=True', 'datetime': '2025-04-21T23:56:58.154549', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2025-04-21 23:56:58,434 : INFO : EPOCH 0: training on 169082 raw words (145340 effective words) took 0.3s, 538236 effective words/s\n",
      "2025-04-21 23:56:58,776 : INFO : EPOCH 1: training on 169082 raw words (145394 effective words) took 0.3s, 465630 effective words/s\n",
      "2025-04-21 23:56:59,053 : INFO : EPOCH 2: training on 169082 raw words (145233 effective words) took 0.3s, 516360 effective words/s\n",
      "2025-04-21 23:56:59,384 : INFO : EPOCH 3: training on 169082 raw words (145066 effective words) took 0.3s, 474312 effective words/s\n",
      "2025-04-21 23:56:59,668 : INFO : EPOCH 4: training on 169082 raw words (145147 effective words) took 0.2s, 590099 effective words/s\n",
      "2025-04-21 23:56:59,957 : INFO : EPOCH 5: training on 169082 raw words (145336 effective words) took 0.3s, 531961 effective words/s\n",
      "2025-04-21 23:57:00,253 : INFO : EPOCH 6: training on 169082 raw words (145225 effective words) took 0.3s, 511444 effective words/s\n",
      "2025-04-21 23:57:00,543 : INFO : EPOCH 7: training on 169082 raw words (145322 effective words) took 0.3s, 538678 effective words/s\n",
      "2025-04-21 23:57:00,848 : INFO : EPOCH 8: training on 169082 raw words (145247 effective words) took 0.3s, 492079 effective words/s\n",
      "2025-04-21 23:57:01,149 : INFO : EPOCH 9: training on 169082 raw words (145253 effective words) took 0.3s, 521098 effective words/s\n",
      "2025-04-21 23:57:01,463 : INFO : EPOCH 10: training on 169082 raw words (145178 effective words) took 0.3s, 497498 effective words/s\n",
      "2025-04-21 23:57:01,732 : INFO : EPOCH 11: training on 169082 raw words (145347 effective words) took 0.3s, 556430 effective words/s\n",
      "2025-04-21 23:57:02,004 : INFO : EPOCH 12: training on 169082 raw words (145255 effective words) took 0.3s, 563705 effective words/s\n",
      "2025-04-21 23:57:02,295 : INFO : EPOCH 13: training on 169082 raw words (145269 effective words) took 0.3s, 551990 effective words/s\n",
      "2025-04-21 23:57:02,599 : INFO : EPOCH 14: training on 169082 raw words (145441 effective words) took 0.3s, 500887 effective words/s\n",
      "2025-04-21 23:57:02,933 : INFO : EPOCH 15: training on 169082 raw words (145235 effective words) took 0.3s, 465340 effective words/s\n",
      "2025-04-21 23:57:03,266 : INFO : EPOCH 16: training on 169082 raw words (145223 effective words) took 0.3s, 449175 effective words/s\n",
      "2025-04-21 23:57:03,581 : INFO : EPOCH 17: training on 169082 raw words (145199 effective words) took 0.3s, 482165 effective words/s\n",
      "2025-04-21 23:57:03,849 : INFO : EPOCH 18: training on 169082 raw words (145244 effective words) took 0.3s, 567229 effective words/s\n",
      "2025-04-21 23:57:04,136 : INFO : EPOCH 19: training on 169082 raw words (145258 effective words) took 0.3s, 542280 effective words/s\n",
      "2025-04-21 23:57:04,411 : INFO : EPOCH 20: training on 169082 raw words (145321 effective words) took 0.3s, 559041 effective words/s\n",
      "2025-04-21 23:57:04,699 : INFO : EPOCH 21: training on 169082 raw words (145142 effective words) took 0.3s, 549424 effective words/s\n",
      "2025-04-21 23:57:04,965 : INFO : EPOCH 22: training on 169082 raw words (145231 effective words) took 0.3s, 558264 effective words/s\n",
      "2025-04-21 23:57:05,248 : INFO : EPOCH 23: training on 169082 raw words (145155 effective words) took 0.3s, 552483 effective words/s\n",
      "2025-04-21 23:57:05,531 : INFO : EPOCH 24: training on 169082 raw words (145312 effective words) took 0.3s, 552880 effective words/s\n",
      "2025-04-21 23:57:05,800 : INFO : EPOCH 25: training on 169082 raw words (145180 effective words) took 0.3s, 557163 effective words/s\n",
      "2025-04-21 23:57:06,101 : INFO : EPOCH 26: training on 169082 raw words (145149 effective words) took 0.2s, 600760 effective words/s\n",
      "2025-04-21 23:57:06,387 : INFO : EPOCH 27: training on 169082 raw words (145237 effective words) took 0.3s, 553608 effective words/s\n",
      "2025-04-21 23:57:06,658 : INFO : EPOCH 28: training on 169082 raw words (145224 effective words) took 0.3s, 569116 effective words/s\n",
      "2025-04-21 23:57:06,919 : INFO : EPOCH 29: training on 169082 raw words (145324 effective words) took 0.3s, 562736 effective words/s\n",
      "2025-04-21 23:57:07,202 : INFO : EPOCH 30: training on 169082 raw words (145119 effective words) took 0.3s, 539151 effective words/s\n",
      "2025-04-21 23:57:07,502 : INFO : EPOCH 31: training on 169082 raw words (145063 effective words) took 0.3s, 522140 effective words/s\n",
      "2025-04-21 23:57:07,782 : INFO : EPOCH 32: training on 169082 raw words (145372 effective words) took 0.3s, 540381 effective words/s\n",
      "2025-04-21 23:57:08,086 : INFO : EPOCH 33: training on 169082 raw words (145284 effective words) took 0.3s, 503402 effective words/s\n",
      "2025-04-21 23:57:08,368 : INFO : EPOCH 34: training on 169082 raw words (145129 effective words) took 0.3s, 565177 effective words/s\n",
      "2025-04-21 23:57:08,669 : INFO : EPOCH 35: training on 169082 raw words (145377 effective words) took 0.3s, 485619 effective words/s\n",
      "2025-04-21 23:57:08,992 : INFO : EPOCH 36: training on 169082 raw words (145136 effective words) took 0.3s, 501014 effective words/s\n",
      "2025-04-21 23:57:09,250 : INFO : EPOCH 37: training on 169082 raw words (145147 effective words) took 0.3s, 576439 effective words/s\n",
      "2025-04-21 23:57:09,531 : INFO : EPOCH 38: training on 169082 raw words (145133 effective words) took 0.3s, 551490 effective words/s\n",
      "2025-04-21 23:57:09,832 : INFO : EPOCH 39: training on 169082 raw words (145214 effective words) took 0.3s, 509901 effective words/s\n",
      "2025-04-21 23:57:09,832 : INFO : Word2Vec lifecycle event {'msg': 'training on 6763280 raw words (5809461 effective words) took 11.7s, 497051 effective words/s', 'datetime': '2025-04-21T23:57:09.832344', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2025-04-21 23:57:09,832 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11221, vector_size=100, alpha=0.025>', 'datetime': '2025-04-21T23:57:09.832344', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n",
      "2025-04-21 23:57:09,853 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'cre_all_word2vec1cS1.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-04-21T23:57:09.853853', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
      "2025-04-21 23:57:09,854 : INFO : not storing attribute cum_table\n",
      "2025-04-21 23:57:09,969 : INFO : saved cre_all_word2vec1cS1.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  # è¾“å‡ºæ—¥å¿—ä¿¡æ¯\n",
    "# sentences = word2vec.Text8Corpus('texts')  # å°†è¯­æ–™ä¿å­˜åœ¨sentenceä¸­\n",
    "model = word2vec.Word2Vec(texts, sg=1, vector_size=100,  window=5,  min_count=2,  negative=0, sample=0.001, hs=1, workers=4, epochs=40)  # ç”Ÿæˆè¯å‘é‡ç©ºé—´æ¨¡å‹\n",
    "model.save('cre_all_word2vec1cS1.model')  # ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15497a4f-f073-47db-a2c7-9a60b79c2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:58:27,369 : INFO : loading Word2Vec object from cre_all_word2vec1cS1.model\n",
      "2025-04-21 23:58:27,464 : INFO : loading wv recursively from cre_all_word2vec1cS1.model.wv.* with mmap=None\n",
      "2025-04-21 23:58:27,464 : INFO : setting ignored attribute cum_table to None\n",
      "2025-04-21 23:58:27,464 : INFO : Word2Vec lifecycle event {'fname': 'cre_all_word2vec1cS1.model', 'datetime': '2025-04-21T23:58:27.464132', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping: 'ç¥æ´²' not in vocabulary\n",
      "âš ï¸ Skipping: 'è‹±å‰åˆ©' not in vocabulary\n",
      "\n",
      "âœ… Final DataFrame:\n",
      "            ä¸­åœ‹           ä¸­è¯            æ”¯é‚£            æ—¥æœ¬            æ±æ´‹  \\\n",
      "0  ç¾©å£« (0.6281)  è½æˆ (0.5019)   å¸†èˆ¹ (0.5389)  æ­£ç¶“äº‹ (0.5836)    è»Š (0.4739)   \n",
      "1  æ•¬é‡ (0.5586)  æ…¶ç¥ (0.4963)   æ°´æ‰‹ (0.4871)   ç•™å­¸ (0.5741)   å›åˆ° (0.4294)   \n",
      "2  ç¯€ä»¤ (0.5353)  ç‰ˆåœ– (0.4168)  å°åº¦æ´‹ (0.4578)   å¾‹ä¾‹ (0.5064)  å·´ä¸å¾— (0.4279)   \n",
      "3  æ”¹ç”¨ (0.5217)  äº”æ´² (0.4156)    è¼ª (0.4249)   è²·è¾¦ (0.4808)   è·³ä¸Š (0.4250)   \n",
      "4  ç™¾å§“ (0.5159)  å½©çƒ (0.4048)    èˆ¹ (0.4236)  å°è€å©† (0.4498)  é›œè²¨è¡Œ (0.4233)   \n",
      "\n",
      "            è‹±åœ‹             è‹±å€«           ä¸–ç•Œ             è¬åœ‹           å¤©ä¸‹  \n",
      "0  éƒµèˆ¹ (0.5936)    å•†è¼ª (0.5424)  é½·é½ª (0.5479)  æ•´æ•´é½Šé½Š (0.5200)  æ¢éšª (0.4606)  \n",
      "1  ä¸Šå­¸ (0.5269)     çˆ² (0.4704)  å…‰æ˜ (0.5462)    å…¬ç½ª (0.4993)  äº”æ´² (0.4522)  \n",
      "2  ç´ç´„ (0.5003)    èµ«èµ« (0.4598)  æ–‡æ˜ (0.5237)    ç…§æœƒ (0.4503)   åœ‹ (0.4473)  \n",
      "3  å£«å®˜ (0.4949)    å¤±æ•— (0.4556)  åœ°è³ª (0.5132)    æ•é ­ (0.4348)   é¡˜ (0.3836)  \n",
      "4  å¿…åˆ° (0.4765)  å—å¤ªå¹³æ´‹ (0.4538)  éª¯é«’ (0.4813)    ä¿è­· (0.4333)  å¯¥å¯¥ (0.3762)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"cre_all_word2vec1cS1.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['ä¸­åœ‹', 'ä¸­è¯', 'æ”¯é‚£', 'ç¥æ´²', 'æ—¥æœ¬', 'æ±æ´‹', 'è‹±åœ‹', 'è‹±å€«', 'è‹±å‰åˆ©', 'ä¸–ç•Œ', 'è¬åœ‹', 'å¤©ä¸‹']\n",
    "  # â† some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"âš ï¸ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "df.to_excel(\"word2vec_top30_filtered_S7.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nâœ… Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b16401-1b75-42d8-a971-e0f20d93b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"cre_all_word2vec1cS1.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['éæ´²', 'äºéåˆ©åŠ ', 'æ–æ´²', 'é˜¿éåˆ©åŠ ', 'è²å·', 'å—æ´‹', 'è¥¿åŸŸ', 'æ­æ´²', 'æ­ç¾…å·´æ´²', 'äºæ±', 'äºæ´²', 'äºç´°äº', 'äºè¥¿äºæ´²', 'äºè¥¿äº', 'äºç´°äºæ´²']\n",
    "  # â† some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"âš ï¸ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "#df.to_excel(\"word2vec_top30_filtered_S_regions1.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nâœ… Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aacfa-16c0-437c-b82c-48c076da134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"cre_all_word2vec1cS.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['ä¸Šæµ·', 'ç´ç´„', 'å€«æ•¦', 'å­Ÿè²·', 'é¦™æ¸¯', 'æ±äº¬']\n",
    "  # â† some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"âš ï¸ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "#df.to_excel(\"word2vec_top30_filtered_S_cities.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nâœ… Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb562e30-349e-4c5c-ac67-5671f773abbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
