{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d7fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok/fine': [['商品',\n",
       "   '和',\n",
       "   '服务',\n",
       "   '。',\n",
       "   '晓美焰',\n",
       "   '来到',\n",
       "   '北京',\n",
       "   '立方庭',\n",
       "   '参观',\n",
       "   '自然',\n",
       "   '语义',\n",
       "   '科技',\n",
       "   '公司',\n",
       "   '。'],\n",
       "  ['我', '爸', '是', '李刚']],\n",
       " 'pos/863': [['n',\n",
       "   'c',\n",
       "   'v',\n",
       "   'w',\n",
       "   'nh',\n",
       "   'v',\n",
       "   'ns',\n",
       "   'ni',\n",
       "   'v',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'w'],\n",
       "  ['r', 'n', 'v', 'nh']]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hanlp_restful import HanLPClient\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import glob\n",
    "\n",
    "HanLP = HanLPClient('https://www.hanlp.com/api', auth=\"\", language='zh')\n",
    "HanLP.parse([\"商品和服务。晓美焰来到北京立方庭参观自然语义科技公司。\", \"我爸是李刚\"], tasks='pos/863')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91565693-4acf-4366-a54d-7ab91b1fd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 29 stopwords.\n",
      "🔍 Starting to process files...\n",
      "\n",
      "📄 (1) Processing 不可思议.txt...\n",
      "📊 不可思议.txt: 30 sentences, 238 tokens kept.\n",
      "\n",
      "📄 (2) Processing 中央亚非利加之蛮地探险.txt...\n",
      "📊 中央亚非利加之蛮地探险.txt: 199 sentences, 1688 tokens kept.\n",
      "\n",
      "📄 (3) Processing 冒险夜行.txt...\n",
      "📊 冒险夜行.txt: 34 sentences, 293 tokens kept.\n",
      "\n",
      "📄 (4) Processing 冒险奇谈.txt...\n",
      "📊 冒险奇谈.txt: 40 sentences, 390 tokens kept.\n",
      "\n",
      "📄 (5) Processing 冒险小说孤店村.txt...\n",
      "📊 冒险小说孤店村.txt: 80 sentences, 845 tokens kept.\n",
      "\n",
      "📄 (6) Processing 冒险小说柏麦船长航海谈.txt...\n",
      "📊 冒险小说柏麦船长航海谈.txt: 80 sentences, 837 tokens kept.\n",
      "\n",
      "📄 (7) Processing 冒险小说火山岛.txt...\n",
      "📊 冒险小说火山岛.txt: 75 sentences, 821 tokens kept.\n",
      "\n",
      "📄 (8) Processing 冒险小说航海家儿.txt...\n",
      "📊 冒险小说航海家儿.txt: 281 sentences, 2713 tokens kept.\n",
      "\n",
      "📄 (9) Processing 冒险小说金沙窟.txt...\n",
      "📊 冒险小说金沙窟.txt: 90 sentences, 1440 tokens kept.\n",
      "\n",
      "📄 (10) Processing 冒险小说鬼域.txt...\n",
      "📊 冒险小说鬼域.txt: 181 sentences, 1279 tokens kept.\n",
      "\n",
      "📄 (11) Processing 冒险成功之商界伟人.txt...\n",
      "📊 冒险成功之商界伟人.txt: 246 sentences, 2213 tokens kept.\n",
      "\n",
      "📄 (12) Processing 冢中怪.txt...\n",
      "📊 冢中怪.txt: 84 sentences, 996 tokens kept.\n",
      "\n",
      "📄 (13) Processing 北冰洋归客谈.txt...\n",
      "📊 北冰洋归客谈.txt: 22 sentences, 221 tokens kept.\n",
      "\n",
      "📄 (14) Processing 南斐洲脱险记.txt...\n",
      "📊 南斐洲脱险记.txt: 239 sentences, 2029 tokens kept.\n",
      "\n",
      "📄 (15) Processing 叶兰.txt...\n",
      "📊 叶兰.txt: 41 sentences, 373 tokens kept.\n",
      "\n",
      "📄 (16) Processing 司宾洛.txt...\n",
      "📊 司宾洛.txt: 352 sentences, 3555 tokens kept.\n",
      "\n",
      "📄 (17) Processing 回风.txt...\n",
      "📊 回风.txt: 77 sentences, 737 tokens kept.\n",
      "\n",
      "📄 (18) Processing 地下都会与海底森林.txt...\n",
      "📊 地下都会与海底森林.txt: 57 sentences, 631 tokens kept.\n",
      "\n",
      "📄 (19) Processing 女丈夫.txt...\n",
      "📊 女丈夫.txt: 904 sentences, 7423 tokens kept.\n",
      "\n",
      "📄 (20) Processing 小中华.txt...\n",
      "📊 小中华.txt: 278 sentences, 2293 tokens kept.\n",
      "\n",
      "📄 (21) Processing 岛国归婚记.txt...\n",
      "📊 岛国归婚记.txt: 200 sentences, 2096 tokens kept.\n",
      "\n",
      "📄 (22) Processing 弟.txt...\n",
      "📊 弟.txt: 23 sentences, 253 tokens kept.\n",
      "\n",
      "📄 (23) Processing 怪国探险记.txt...\n",
      "📊 怪国探险记.txt: 110 sentences, 981 tokens kept.\n",
      "\n",
      "📄 (24) Processing 探险英雄传.txt...\n",
      "📊 探险英雄传.txt: 300 sentences, 2872 tokens kept.\n",
      "\n",
      "📄 (25) Processing 新殖民地仲尼島.txt...\n",
      "📊 新殖民地仲尼島.txt: 111 sentences, 1019 tokens kept.\n",
      "\n",
      "📄 (26) Processing 月球殖民地小說.txt...\n",
      "📊 月球殖民地小說.txt: 4121 sentences, 38113 tokens kept.\n",
      "\n",
      "📄 (27) Processing 树穴书.txt...\n",
      "📊 树穴书.txt: 141 sentences, 1509 tokens kept.\n",
      "\n",
      "📄 (28) Processing 毒洲探险记.txt...\n",
      "📊 毒洲探险记.txt: 272 sentences, 2780 tokens kept.\n",
      "\n",
      "📄 (29) Processing 海上逸史.txt...\n",
      "📊 海上逸史.txt: 160 sentences, 1594 tokens kept.\n",
      "\n",
      "📄 (30) Processing 海底奇谈.txt...\n",
      "📊 海底奇谈.txt: 2309 sentences, 21535 tokens kept.\n",
      "\n",
      "📄 (31) Processing 渔父冒险记 冠万.txt...\n",
      "📊 渔父冒险记 冠万.txt: 32 sentences, 262 tokens kept.\n",
      "\n",
      "📄 (32) Processing 渔父冒险记 孙明兴.txt...\n",
      "📊 渔父冒险记 孙明兴.txt: 18 sentences, 159 tokens kept.\n",
      "\n",
      "📄 (33) Processing 渔父冒险记 林远隆.txt...\n",
      "📊 渔父冒险记 林远隆.txt: 9 sentences, 78 tokens kept.\n",
      "\n",
      "📄 (34) Processing 渔父冒险记 黄荣仙.txt...\n",
      "📊 渔父冒险记 黄荣仙.txt: 23 sentences, 196 tokens kept.\n",
      "\n",
      "📄 (35) Processing 滕半仙伝.txt...\n",
      "📊 滕半仙伝.txt: 58 sentences, 557 tokens kept.\n",
      "\n",
      "📄 (36) Processing 片帆影.txt...\n",
      "📊 片帆影.txt: 117 sentences, 1262 tokens kept.\n",
      "\n",
      "📄 (37) Processing 狮子血.txt...\n",
      "📊 狮子血.txt: 1011 sentences, 9856 tokens kept.\n",
      "\n",
      "📄 (38) Processing 狼吻余生记.txt...\n",
      "📊 狼吻余生记.txt: 63 sentences, 736 tokens kept.\n",
      "\n",
      "📄 (39) Processing 百斯笃殖民政策记.txt...\n",
      "📊 百斯笃殖民政策记.txt: 193 sentences, 1870 tokens kept.\n",
      "\n",
      "📄 (40) Processing 盗盗.txt...\n",
      "📊 盗盗.txt: 10 sentences, 88 tokens kept.\n",
      "\n",
      "📄 (41) Processing 破舟遇险记.txt...\n",
      "📊 破舟遇险记.txt: 42 sentences, 420 tokens kept.\n",
      "\n",
      "📄 (42) Processing 磊落士官.txt...\n",
      "📊 磊落士官.txt: 101 sentences, 1037 tokens kept.\n",
      "\n",
      "📄 (43) Processing 穴居.txt...\n",
      "📊 穴居.txt: 34 sentences, 394 tokens kept.\n",
      "\n",
      "📄 (44) Processing 空中岛.txt...\n",
      "📊 空中岛.txt: 42 sentences, 457 tokens kept.\n",
      "\n",
      "📄 (45) Processing 纪实冒险小说洞庭湖.txt...\n",
      "📊 纪实冒险小说洞庭湖.txt: 31 sentences, 376 tokens kept.\n",
      "\n",
      "📄 (46) Processing 航海奇遇.txt...\n",
      "📊 航海奇遇.txt: 328 sentences, 2357 tokens kept.\n",
      "\n",
      "📄 (47) Processing 航船盗.txt...\n",
      "📊 航船盗.txt: 127 sentences, 1284 tokens kept.\n",
      "\n",
      "📄 (48) Processing 英男孝女记.txt...\n",
      "📊 英男孝女记.txt: 204 sentences, 1676 tokens kept.\n",
      "\n",
      "📄 (49) Processing 荒岛孤踪.txt...\n",
      "📊 荒岛孤踪.txt: 47 sentences, 494 tokens kept.\n",
      "\n",
      "📄 (50) Processing 荒岛漂泊记.txt...\n",
      "📊 荒岛漂泊记.txt: 117 sentences, 924 tokens kept.\n",
      "\n",
      "📄 (51) Processing 蛇虎斗.txt...\n",
      "📊 蛇虎斗.txt: 85 sentences, 898 tokens kept.\n",
      "\n",
      "📄 (52) Processing 记事小说虎口余生记.txt...\n",
      "📊 记事小说虎口余生记.txt: 63 sentences, 613 tokens kept.\n",
      "\n",
      "📄 (53) Processing 陆治斯南极探险事.txt...\n",
      "📊 陆治斯南极探险事.txt: 25 sentences, 263 tokens kept.\n",
      "\n",
      "📄 (54) Processing 青铜钱.txt...\n",
      "📊 青铜钱.txt: 168 sentences, 1771 tokens kept.\n",
      "\n",
      "📄 (55) Processing 非洲探险记.txt...\n",
      "📊 非洲探险记.txt: 51 sentences, 448 tokens kept.\n",
      "\n",
      "📄 (56) Processing 革命党.txt...\n",
      "📊 革命党.txt: 3096 sentences, 32412 tokens kept.\n",
      "\n",
      "📄 (57) Processing 驮轮御风记.txt...\n",
      "📊 驮轮御风记.txt: 94 sentences, 853 tokens kept.\n",
      "\n",
      "📄 (58) Processing 魔窟.txt...\n",
      "📊 魔窟.txt: 92 sentences, 856 tokens kept.\n",
      "\n",
      "📄 (59) Processing 鱼丽水历险记.txt...\n",
      "📊 鱼丽水历险记.txt: 208 sentences, 1926 tokens kept.\n",
      "\n",
      "📄 (60) Processing 鸭蛋岛探险记.txt...\n",
      "📊 鸭蛋岛探险记.txt: 81 sentences, 792 tokens kept.\n",
      "\n",
      "✅ Token filtering complete.\n",
      "📈 Total sentences: 17707\n",
      "📈 Total tokens: 169082\n",
      "✅ Filtered sentences saved to 'filtered_sentences2.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# === Setup ===\n",
    "root = \"dataclean3\"\n",
    "output_model_path = \"filtered_word2vec_pS.model\"\n",
    "query_timestamps = []\n",
    "texts = []\n",
    "file_stats = dict()\n",
    "\n",
    "# Load stopwords\n",
    "stopword_file = 'stopwords.txt'\n",
    "\n",
    "with open(stopword_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "print(f\"✅ Loaded {len(stopwords)} stopwords.\")\n",
    "\n",
    "valid_pos_tags = {'n', 'v', 'a', 'ns'}\n",
    "\n",
    "MAX_BATCH_SIZE = 200\n",
    "MAX_CALLS_PER_MINUTE = 70\n",
    "TIME_WINDOW = 60\n",
    "\n",
    "# === Helpers ===\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[。！？”」])', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def batch_sentences(sentences, batch_size=200):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i:i+batch_size]\n",
    "\n",
    "def respect_rate_limit():\n",
    "    if len(query_timestamps) >= MAX_CALLS_PER_MINUTE:\n",
    "        gap = time.time() - query_timestamps[-MAX_CALLS_PER_MINUTE]\n",
    "        if gap < TIME_WINDOW:\n",
    "            sleep_time = TIME_WINDOW - gap\n",
    "            print(f\"\\tSleeping {sleep_time:.2f} seconds to respect rate limit...\")\n",
    "            time.sleep(sleep_time)\n",
    "    if len(query_timestamps) > MAX_CALLS_PER_MINUTE:\n",
    "        del query_timestamps[:-MAX_CALLS_PER_MINUTE]\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def filter_tokens(data):\n",
    "    tokens = flatten(data['tok/fine'])\n",
    "    pos_tags = flatten(data['pos/863'])\n",
    "\n",
    "    selected_tokens = []\n",
    "    for token, pos in zip(tokens, pos_tags):\n",
    "        if (pos in valid_pos_tags) and (token not in stopwords):\n",
    "            selected_tokens.append(token)\n",
    "\n",
    "    return selected_tokens\n",
    "\n",
    "def process_batch(batch):\n",
    "    try:\n",
    "        query_timestamps.append(time.time())\n",
    "        data = HanLP.parse(batch, tasks=['tok/fine', 'pos/863'])\n",
    "\n",
    "        batch_filtered = []\n",
    "\n",
    "        for sentence_idx in range(len(batch)):\n",
    "            sentence_data = {\n",
    "                'tok/fine': [data['tok/fine'][sentence_idx]],\n",
    "                'pos/863': [data['pos/863'][sentence_idx]]\n",
    "            }\n",
    "            filtered_tokens = filter_tokens(sentence_data)\n",
    "            if filtered_tokens:\n",
    "                batch_filtered.append(filtered_tokens)\n",
    "\n",
    "        return batch_filtered\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❗ Error processing batch: {e}\")\n",
    "        return []\n",
    "\n",
    "# === Main Loop ===\n",
    "\n",
    "total_tokens_all_files = 0\n",
    "total_sentences_all_files = 0\n",
    "\n",
    "print(\"🔍 Starting to process files...\")\n",
    "\n",
    "for file_idx, file_name in enumerate(os.listdir(root), 1):\n",
    "    if not file_name.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📄 ({file_idx}) Processing {file_name}...\")\n",
    "    file_path = os.path.join(root, file_name)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    file_sentences = []\n",
    "    file_token_count = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(batch_sentences(sentences, batch_size=MAX_BATCH_SIZE)):\n",
    "        respect_rate_limit()\n",
    "        filtered_batch = process_batch(batch)\n",
    "\n",
    "        if filtered_batch:\n",
    "            file_sentences.extend(filtered_batch)\n",
    "\n",
    "    texts.extend(file_sentences)\n",
    "    total_sentences_all_files += len(file_sentences)\n",
    "    file_tokens = sum(len(sentence) for sentence in file_sentences)\n",
    "    total_tokens_all_files += file_tokens\n",
    "\n",
    "    print(f\"📊 {file_name}: {len(file_sentences)} sentences, {file_tokens} tokens kept.\")\n",
    "\n",
    "print(\"\\n✅ Token filtering complete.\")\n",
    "print(f\"📈 Total sentences: {total_sentences_all_files}\")\n",
    "print(f\"📈 Total tokens: {total_tokens_all_files}\")\n",
    "\n",
    "# === Save filtered sentences (optional) ===\n",
    "with open(\"filtered_sentences2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ Filtered sentences saved to 'filtered_sentences2.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825566d-c41f-498b-8004-dc1f18aa7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered_sentences1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8113dd2-fbd9-4308-9858-9d05638bcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  # 输出日志信息\n",
    "# sentences = word2vec.Text8Corpus('texts')  # 将语料保存在sentence中\n",
    "model = word2vec.Word2Vec(texts, sg=1, vector_size=100,  window=5,  min_count=2,  negative=0, sample=0.001, hs=1, workers=4, epochs=20)  # 生成词向量空间模型\n",
    "model.save('cre_all_word2vec1cS.model')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe8f7aa-7f1d-48d8-b653-cf728f158348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:56:57,549 : INFO : collecting all words and their counts\n",
      "2025-04-21 23:56:57,550 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-21 23:56:57,574 : INFO : PROGRESS: at sentence #10000, processed 92828 words, keeping 15917 word types\n",
      "2025-04-21 23:56:57,589 : INFO : collected 22484 word types from a corpus of 169082 raw words and 17707 sentences\n",
      "2025-04-21 23:56:57,589 : INFO : Creating a fresh vocabulary\n",
      "2025-04-21 23:56:57,638 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11221 unique words (49.91% of original 22484, drops 11263)', 'datetime': '2025-04-21T23:56:57.638369', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,638 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 157819 word corpus (93.34% of original 169082, drops 11263)', 'datetime': '2025-04-21T23:56:57.638369', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,743 : INFO : deleting the raw counts dictionary of 22484 items\n",
      "2025-04-21 23:56:57,744 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2025-04-21 23:56:57,745 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 145237.87325628963 word corpus (92.0%% of prior 157819)', 'datetime': '2025-04-21T23:56:57.745427', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-21 23:56:57,751 : INFO : constructing a huffman tree from 11221 words\n",
      "2025-04-21 23:56:58,134 : INFO : built huffman tree with maximum node depth 16\n",
      "2025-04-21 23:56:58,147 : INFO : estimated required memory for 11221 words and 100 dimensions: 16831500 bytes\n",
      "2025-04-21 23:56:58,148 : INFO : resetting layer weights\n",
      "2025-04-21 23:56:58,154 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-21T23:56:58.154549', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2025-04-21 23:56:58,154 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 11221 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=0 window=5 shrink_windows=True', 'datetime': '2025-04-21T23:56:58.154549', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2025-04-21 23:56:58,434 : INFO : EPOCH 0: training on 169082 raw words (145340 effective words) took 0.3s, 538236 effective words/s\n",
      "2025-04-21 23:56:58,776 : INFO : EPOCH 1: training on 169082 raw words (145394 effective words) took 0.3s, 465630 effective words/s\n",
      "2025-04-21 23:56:59,053 : INFO : EPOCH 2: training on 169082 raw words (145233 effective words) took 0.3s, 516360 effective words/s\n",
      "2025-04-21 23:56:59,384 : INFO : EPOCH 3: training on 169082 raw words (145066 effective words) took 0.3s, 474312 effective words/s\n",
      "2025-04-21 23:56:59,668 : INFO : EPOCH 4: training on 169082 raw words (145147 effective words) took 0.2s, 590099 effective words/s\n",
      "2025-04-21 23:56:59,957 : INFO : EPOCH 5: training on 169082 raw words (145336 effective words) took 0.3s, 531961 effective words/s\n",
      "2025-04-21 23:57:00,253 : INFO : EPOCH 6: training on 169082 raw words (145225 effective words) took 0.3s, 511444 effective words/s\n",
      "2025-04-21 23:57:00,543 : INFO : EPOCH 7: training on 169082 raw words (145322 effective words) took 0.3s, 538678 effective words/s\n",
      "2025-04-21 23:57:00,848 : INFO : EPOCH 8: training on 169082 raw words (145247 effective words) took 0.3s, 492079 effective words/s\n",
      "2025-04-21 23:57:01,149 : INFO : EPOCH 9: training on 169082 raw words (145253 effective words) took 0.3s, 521098 effective words/s\n",
      "2025-04-21 23:57:01,463 : INFO : EPOCH 10: training on 169082 raw words (145178 effective words) took 0.3s, 497498 effective words/s\n",
      "2025-04-21 23:57:01,732 : INFO : EPOCH 11: training on 169082 raw words (145347 effective words) took 0.3s, 556430 effective words/s\n",
      "2025-04-21 23:57:02,004 : INFO : EPOCH 12: training on 169082 raw words (145255 effective words) took 0.3s, 563705 effective words/s\n",
      "2025-04-21 23:57:02,295 : INFO : EPOCH 13: training on 169082 raw words (145269 effective words) took 0.3s, 551990 effective words/s\n",
      "2025-04-21 23:57:02,599 : INFO : EPOCH 14: training on 169082 raw words (145441 effective words) took 0.3s, 500887 effective words/s\n",
      "2025-04-21 23:57:02,933 : INFO : EPOCH 15: training on 169082 raw words (145235 effective words) took 0.3s, 465340 effective words/s\n",
      "2025-04-21 23:57:03,266 : INFO : EPOCH 16: training on 169082 raw words (145223 effective words) took 0.3s, 449175 effective words/s\n",
      "2025-04-21 23:57:03,581 : INFO : EPOCH 17: training on 169082 raw words (145199 effective words) took 0.3s, 482165 effective words/s\n",
      "2025-04-21 23:57:03,849 : INFO : EPOCH 18: training on 169082 raw words (145244 effective words) took 0.3s, 567229 effective words/s\n",
      "2025-04-21 23:57:04,136 : INFO : EPOCH 19: training on 169082 raw words (145258 effective words) took 0.3s, 542280 effective words/s\n",
      "2025-04-21 23:57:04,411 : INFO : EPOCH 20: training on 169082 raw words (145321 effective words) took 0.3s, 559041 effective words/s\n",
      "2025-04-21 23:57:04,699 : INFO : EPOCH 21: training on 169082 raw words (145142 effective words) took 0.3s, 549424 effective words/s\n",
      "2025-04-21 23:57:04,965 : INFO : EPOCH 22: training on 169082 raw words (145231 effective words) took 0.3s, 558264 effective words/s\n",
      "2025-04-21 23:57:05,248 : INFO : EPOCH 23: training on 169082 raw words (145155 effective words) took 0.3s, 552483 effective words/s\n",
      "2025-04-21 23:57:05,531 : INFO : EPOCH 24: training on 169082 raw words (145312 effective words) took 0.3s, 552880 effective words/s\n",
      "2025-04-21 23:57:05,800 : INFO : EPOCH 25: training on 169082 raw words (145180 effective words) took 0.3s, 557163 effective words/s\n",
      "2025-04-21 23:57:06,101 : INFO : EPOCH 26: training on 169082 raw words (145149 effective words) took 0.2s, 600760 effective words/s\n",
      "2025-04-21 23:57:06,387 : INFO : EPOCH 27: training on 169082 raw words (145237 effective words) took 0.3s, 553608 effective words/s\n",
      "2025-04-21 23:57:06,658 : INFO : EPOCH 28: training on 169082 raw words (145224 effective words) took 0.3s, 569116 effective words/s\n",
      "2025-04-21 23:57:06,919 : INFO : EPOCH 29: training on 169082 raw words (145324 effective words) took 0.3s, 562736 effective words/s\n",
      "2025-04-21 23:57:07,202 : INFO : EPOCH 30: training on 169082 raw words (145119 effective words) took 0.3s, 539151 effective words/s\n",
      "2025-04-21 23:57:07,502 : INFO : EPOCH 31: training on 169082 raw words (145063 effective words) took 0.3s, 522140 effective words/s\n",
      "2025-04-21 23:57:07,782 : INFO : EPOCH 32: training on 169082 raw words (145372 effective words) took 0.3s, 540381 effective words/s\n",
      "2025-04-21 23:57:08,086 : INFO : EPOCH 33: training on 169082 raw words (145284 effective words) took 0.3s, 503402 effective words/s\n",
      "2025-04-21 23:57:08,368 : INFO : EPOCH 34: training on 169082 raw words (145129 effective words) took 0.3s, 565177 effective words/s\n",
      "2025-04-21 23:57:08,669 : INFO : EPOCH 35: training on 169082 raw words (145377 effective words) took 0.3s, 485619 effective words/s\n",
      "2025-04-21 23:57:08,992 : INFO : EPOCH 36: training on 169082 raw words (145136 effective words) took 0.3s, 501014 effective words/s\n",
      "2025-04-21 23:57:09,250 : INFO : EPOCH 37: training on 169082 raw words (145147 effective words) took 0.3s, 576439 effective words/s\n",
      "2025-04-21 23:57:09,531 : INFO : EPOCH 38: training on 169082 raw words (145133 effective words) took 0.3s, 551490 effective words/s\n",
      "2025-04-21 23:57:09,832 : INFO : EPOCH 39: training on 169082 raw words (145214 effective words) took 0.3s, 509901 effective words/s\n",
      "2025-04-21 23:57:09,832 : INFO : Word2Vec lifecycle event {'msg': 'training on 6763280 raw words (5809461 effective words) took 11.7s, 497051 effective words/s', 'datetime': '2025-04-21T23:57:09.832344', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2025-04-21 23:57:09,832 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=11221, vector_size=100, alpha=0.025>', 'datetime': '2025-04-21T23:57:09.832344', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n",
      "2025-04-21 23:57:09,853 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'cre_all_word2vec1cS1.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-04-21T23:57:09.853853', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
      "2025-04-21 23:57:09,854 : INFO : not storing attribute cum_table\n",
      "2025-04-21 23:57:09,969 : INFO : saved cre_all_word2vec1cS1.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  # 输出日志信息\n",
    "# sentences = word2vec.Text8Corpus('texts')  # 将语料保存在sentence中\n",
    "model = word2vec.Word2Vec(texts, sg=1, vector_size=100,  window=5,  min_count=2,  negative=0, sample=0.001, hs=1, workers=4, epochs=40)  # 生成词向量空间模型\n",
    "model.save('cre_all_word2vec1cS1.model')  # 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15497a4f-f073-47db-a2c7-9a60b79c2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:58:27,369 : INFO : loading Word2Vec object from cre_all_word2vec1cS1.model\n",
      "2025-04-21 23:58:27,464 : INFO : loading wv recursively from cre_all_word2vec1cS1.model.wv.* with mmap=None\n",
      "2025-04-21 23:58:27,464 : INFO : setting ignored attribute cum_table to None\n",
      "2025-04-21 23:58:27,464 : INFO : Word2Vec lifecycle event {'fname': 'cre_all_word2vec1cS1.model', 'datetime': '2025-04-21T23:58:27.464132', 'gensim': '4.3.2', 'python': '3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping: '神洲' not in vocabulary\n",
      "⚠️ Skipping: '英吉利' not in vocabulary\n",
      "\n",
      "✅ Final DataFrame:\n",
      "            中國           中華            支那            日本            東洋  \\\n",
      "0  義士 (0.6281)  落成 (0.5019)   帆船 (0.5389)  正經事 (0.5836)    車 (0.4739)   \n",
      "1  敬重 (0.5586)  慶祝 (0.4963)   水手 (0.4871)   留學 (0.5741)   回到 (0.4294)   \n",
      "2  節令 (0.5353)  版圖 (0.4168)  印度洋 (0.4578)   律例 (0.5064)  巴不得 (0.4279)   \n",
      "3  改用 (0.5217)  五洲 (0.4156)    輪 (0.4249)   買辦 (0.4808)   跳上 (0.4250)   \n",
      "4  百姓 (0.5159)  彩球 (0.4048)    船 (0.4236)  小老婆 (0.4498)  雜貨行 (0.4233)   \n",
      "\n",
      "            英國             英倫           世界             萬國           天下  \n",
      "0  郵船 (0.5936)    商輪 (0.5424)  齷齪 (0.5479)  整整齊齊 (0.5200)  探險 (0.4606)  \n",
      "1  上學 (0.5269)     爲 (0.4704)  光明 (0.5462)    公罪 (0.4993)  五洲 (0.4522)  \n",
      "2  紐約 (0.5003)    赫赫 (0.4598)  文明 (0.5237)    照會 (0.4503)   國 (0.4473)  \n",
      "3  士官 (0.4949)    失敗 (0.4556)  地質 (0.5132)    捕頭 (0.4348)   願 (0.3836)  \n",
      "4  必到 (0.4765)  南太平洋 (0.4538)  骯髒 (0.4813)    保護 (0.4333)  寥寥 (0.3762)  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"cre_all_word2vec1cS1.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['中國', '中華', '支那', '神洲', '日本', '東洋', '英國', '英倫', '英吉利', '世界', '萬國', '天下']\n",
    "  # ← some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "df.to_excel(\"word2vec_top30_filtered_S7.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\n✅ Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b16401-1b75-42d8-a971-e0f20d93b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"cre_all_word2vec1cS1.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['非洲', '亞非利加', '斐洲', '阿非利加', '菲州', '南洋', '西域', '歐洲', '歐羅巴洲', '亞東', '亞洲', '亞細亞', '亞西亞洲', '亞西亞', '亞細亞洲']\n",
    "  # ← some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "#df.to_excel(\"word2vec_top30_filtered_S_regions1.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\n✅ Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aacfa-16c0-437c-b82c-48c076da134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"cre_all_word2vec1cS.model\")\n",
    "\n",
    "# Define target words\n",
    "target_words = ['上海', '紐約', '倫敦', '孟買', '香港', '東京']\n",
    "  # ← some might not be in the vocab\n",
    "topn = 30\n",
    "\n",
    "# Dictionary to hold formatted results\n",
    "results_dict = {}\n",
    "\n",
    "# Collect most similar words for each valid word\n",
    "for word in target_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        formatted = [f\"{token} ({score:.4f})\" for token, score in similar]\n",
    "        results_dict[word] = formatted\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping: '{word}' not in vocabulary\")\n",
    "\n",
    "# Create DataFrame only from valid words\n",
    "df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save to Excel (or CSV if you prefer)\n",
    "#df.to_excel(\"word2vec_top30_filtered_S_cities.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"\\n✅ Final DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb562e30-349e-4c5c-ac67-5671f773abbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
